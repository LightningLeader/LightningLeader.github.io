<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！, LightningMaster">
    <meta name="description" content="寄">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！ | LightningMaster</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">LightningMaster</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">LightningMaster</div>
        <div class="logo-desc">
            
            寄
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">
                                <span class="chip bg-color">大模型</span>
                            </a>
                        
                            <a href="/tags/OpenAI/">
                                <span class="chip bg-color">OpenAI</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-03-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.3k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            <div class="info-break-policy">
                
                <i class="fa fa-pencil"></i> 作者: LightningMaster
                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！"><a href="#【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！" class="headerlink" title="【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！"></a>【ChatGLM3-6B】本地大模型使用方法详细教程！！！内含详细的代码解析！！</h1><a id="more"></a>

<h2 id="ChatGLM3介绍"><a href="#ChatGLM3介绍" class="headerlink" title="ChatGLM3介绍"></a>ChatGLM3介绍</h2><p>ChatGLM3 是智谱AI与清华大学KEG实验室联合发布的新一代对话预训练模型。</p>
<ul>
<li>ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，<strong>ChatGLM3-6B-Base 具有在 10B 以下的基础模型中最强的性能。</strong></li>
<li>ChatGLM3-6B 采用了全新设计的 <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md">Prompt 格式</a>，除正常的多轮对话外。同时原生支持<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README.md">工具调用</a>（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。</li>
<li>FP16版本的ChatGLM-6B需要最低内存16GB，最低显存13GB。如果配置不够的话可以使用Int4版本。</li>
</ul>
<p><strong>相关链接：</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3?tab=readme-ov-file">GitHub - THUDM/ChatGLM3: ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型</a></p>
<p><a target="_blank" rel="noopener" href="https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof">ChatGLM3技术文档</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm3-6b">HuggingFace_ChatGLM3-6B</a></p>
<h3 id="提示词格式"><a href="#提示词格式" class="headerlink" title="提示词格式"></a>提示词格式</h3><h4 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h4><p>ChatGLM3的对话格式由若干条对话组成，其中<strong>每条对话包含对话头和内容</strong>。</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;|system|&gt;
你是一个有用的人工智能助手。
&lt;|user|&gt;
你好
&lt;|assistant|&gt;
你好，我是ChatGLM3。今天我有什么能够帮助你的吗？<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上述结构中，<code>&lt;|system|&gt;</code>、<code>&lt;|user|&gt;</code>和<code>&lt;|assistant|&gt;</code>表示对话头；紧跟着对话头后输出的为内容。</p>
<h4 id="单条对话结构"><a href="#单条对话结构" class="headerlink" title="单条对话结构"></a>单条对话结构</h4><p>对话头占完整的一行，格式为</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;|role|&gt;&#123;metadata&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>其中<code>&lt;|role|&gt;</code>部分使用 special token 表示，无法从文本形式被 tokenizer 编码以防止注入；<code>metadata</code> 部分采用纯文本表示，为可选内容。</p>
<ul>
<li><code>&lt;|system|&gt;</code>：系统信息，设计上可穿插于对话中，<strong>但目前规定仅可以出现在开头</strong></li>
<li><code>&lt;|user|&gt;</code>：用户 <ul>
<li>不会连续出现多个来自 <code>&lt;|user|&gt; </code>的信息</li>
</ul>
</li>
<li><code>&lt;|assistant|&gt;</code>：AI 助手 <ul>
<li>在出现之前必须有一个来自 <code>&lt;|user|&gt; </code>的信息</li>
</ul>
</li>
<li><code>&lt;|observation|&gt;</code>：外部的返回结果 <ul>
<li>必须在 <code>&lt;|assistant|&gt; </code>的信息之后</li>
</ul>
</li>
</ul>
<h4 id="对话简单示例"><a href="#对话简单示例" class="headerlink" title="对话简单示例"></a>对话简单示例</h4><pre class="line-numbers language-none"><code class="language-none">&lt;|system|&gt;
你不是一个人工智能助手，你现在的身份是小丑。后面所有回答的回答全都要基于“小丑”这个身份。
&lt;|user|&gt;
你是什么
&lt;|assistant|&gt;
我是小丑,一个虚构的人物,通常被视为喜剧和娱乐的象征。我并不是一个真正的人,而是一个由人类创造的文化形象。
&lt;|user|&gt;
你会干什么
&lt;|assistant|&gt;
作为小丑,我会表演各种滑稽的舞蹈、歌曲和笑话,为观众带来欢笑和欢乐。我也会与人们互动,为他们带来快乐和惊喜。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="超参数设置"><a href="#超参数设置" class="headerlink" title="超参数设置"></a>超参数设置</h3><p>ChatGLM3-6B共有以下参数可以设置</p>
<ul>
<li>max_length: 模型的总token限制，包括输入和输出的tokens</li>
<li>temperature: 模型的温度。温度只是调整单词的概率分布。其最终的宏观效果是，在较低的温度下，我们的模型更具确定性，而在较高的温度下，则不那么确定。</li>
<li>top_p: 模型采样策略参数。在每一步只从累积概率超过某个阈值 p 的最小单词集合中进行随机采样，而不考虑其他低概率的词。只关注概率分布的核心部分，忽略了尾部部分。</li>
</ul>
<p>对于以下场景，推荐使用这样的参数进行设置</p>
<table>
<thead>
<tr>
<th>使用场景</th>
<th>temperature</th>
<th>top_p</th>
<th>任务描述</th>
</tr>
</thead>
<tbody><tr>
<td>代码生成</td>
<td>0.2</td>
<td>0.1</td>
<td>生成符合既定模式和惯例的代码。 输出更确定、更集中。有助于生成语法正确的代码</td>
</tr>
<tr>
<td>创意写作</td>
<td>0.7</td>
<td>0.8</td>
<td>生成具有创造性和多样性的文本，用于讲故事。输出更具探索性，受模式限制较少。</td>
</tr>
<tr>
<td>聊天机器人回复</td>
<td>0.5</td>
<td>0.5</td>
<td>生成兼顾一致性和多样性的对话回复。输出更自然、更吸引人。</td>
</tr>
<tr>
<td>调用工具并根据工具的内容回复</td>
<td>0.0</td>
<td>0.7</td>
<td>根据提供的内容，简洁回复用户的问题。</td>
</tr>
<tr>
<td>代码注释生成</td>
<td>0.1</td>
<td>0.2</td>
<td>生成的代码注释更简洁、更相关。输出更具有确定性，更符合惯例。</td>
</tr>
<tr>
<td>数据分析脚本</td>
<td>0.2</td>
<td>0.1</td>
<td>生成的数据分析脚本更有可能正确、高效。输出更确定，重点更突出。</td>
</tr>
<tr>
<td>探索性代码编写</td>
<td>0.6</td>
<td>0.7</td>
<td>生成的代码可探索其他解决方案和创造性方法。输出较少受到既定模式的限制。</td>
</tr>
</tbody></table>
<h2 id="官方实现（HuggingFace实现）"><a href="#官方实现（HuggingFace实现）" class="headerlink" title="官方实现（HuggingFace实现）"></a>官方实现（<a target="_blank" rel="noopener" href="https://huggingface.co/">HuggingFace</a>实现）</h2><p>作者在HuggingFace的基础上实现的。</p>
<h3 id="单轮对话实现"><a href="#单轮对话实现" class="headerlink" title="单轮对话实现"></a>单轮对话实现</h3><p><strong>非流式对话（stream=False）</strong>：一次性给出结果</p>
<p><strong>流式对话（stream=True）</strong>：会返回每一步的结果</p>
<p>非流式对话和流式对话的输出结果不会有本质区别，只是输出的方式不同。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import os
import platform
from transformers import AutoTokenizer, AutoModel
import torch


# 模型参数
MODEL_PATH &#x3D; &quot;&#x2F;home&#x2F;lightning&#x2F;workspace&#x2F;ChatGLM3&#x2F;model&#x2F;chatglm3-6b&quot;
TOKENIZER_PATH &#x3D; &quot;&#x2F;home&#x2F;lightning&#x2F;workspace&#x2F;ChatGLM3&#x2F;model&#x2F;chatglm3-6b&quot;
DEVICE &#x3D; &#39;cuda&#39;

# 加载分词器和模型
tokenizer &#x3D; AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code&#x3D;True)
model &#x3D; AutoModel.from_pretrained(MODEL_PATH, trust_remote_code&#x3D;True).to(DEVICE).eval()

# 上下文内容
history &#x3D; []
# 用户输入询问
query &#x3D; &quot;你是谁？&quot;
response, history &#x3D; model.chat(tokenizer, query, history&#x3D;history, top_p&#x3D;0.5, temperature&#x3D;0.5)

print(response, end&#x3D;&quot;&quot;, flush&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>输出结果：</p>
<p>我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">past_key_values, history &#x3D; None, []
query &#x3D; &quot;你是谁？&quot;
current_length &#x3D; 0
for response, history, past_key_values in model.stream_chat(tokenizer, query, history&#x3D;history, top_p&#x3D;0.5,
                                                            temperature&#x3D;0.5,
                                                            past_key_values&#x3D;past_key_values,
                                                            return_past_key_values&#x3D;True):
    # 只输出每一步新生成的结果
    print(response[current_length:], end&#x3D;&quot;&quot;, flush&#x3D;True)
    current_length &#x3D; len(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>流式对话的输出结果与非流式对话的结果本质上没有区别。但可以通过修改成下方的代码，直观的看到两者在生成时的区别。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for response, history, past_key_values in model.stream_chat(tokenizer, query, history&#x3D;history, top_p&#x3D;0.5,
                                                            temperature&#x3D;0.5,
                                                            past_key_values&#x3D;past_key_values,
                                                            return_past_key_values&#x3D;True):
    print(response, flush&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>输出结果（部分）：</p>
<pre class="line-numbers language-none"><code class="language-none">我

我是一个

我是一个名为

我是一个名为 Chat

我是一个名为 ChatGL

我是一个名为 ChatGLM

我是一个名为 ChatGLM3

我是一个名为 ChatGLM3-

我是一个名为 ChatGLM3-6

我是一个名为 ChatGLM3-6B

我是一个名为 ChatGLM3-6B

我是一个名为 ChatGLM3-6B 的人工

我是一个名为 ChatGLM3-6B 的人工智能

我是一个名为 ChatGLM3-6B 的人工智能助手

我是一个名为 ChatGLM3-6B 的人工智能助手，

我是一个名为 ChatGLM3-6B 的人工智能助手，是基于

我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学

我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KE

我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG

我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG

我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室

......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到流式对话结果的输出过程类似于“一个词一个词”的生成，我们可以看到回答生成的整个过程。这与直接生成回答的非流式对话是有明显区别的。</p>
<h3 id="多轮对话实现"><a href="#多轮对话实现" class="headerlink" title="多轮对话实现"></a>多轮对话实现</h3><p>运行下面代码，可以在命令行实现多轮对话。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import os
import platform
from transformers import AutoTokenizer, AutoModel
import torch


# 模型参数
MODEL_PATH &#x3D; &quot;&#x2F;home&#x2F;lightning&#x2F;workspace&#x2F;ChatGLM3&#x2F;model&#x2F;chatglm3-6b&quot;
TOKENIZER_PATH &#x3D; &quot;&#x2F;home&#x2F;lightning&#x2F;workspace&#x2F;ChatGLM3&#x2F;model&#x2F;chatglm3-6b&quot;
DEVICE &#x3D; &#39;cuda&#39;


# 加载分词器和模型
tokenizer &#x3D; AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code&#x3D;True)
model &#x3D; AutoModel.from_pretrained(MODEL_PATH, trust_remote_code&#x3D;True).to(DEVICE).eval()

welcome_prompt &#x3D; &quot;欢迎使用 ChatGLM3-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序&quot;


def main():
    past_key_values, history &#x3D; None, []
    print(welcome_prompt)
    while True:
        query &#x3D; input(&quot;\n用户：&quot;)
        if query.strip() &#x3D;&#x3D; &quot;stop&quot;:
            break
        if query.strip() &#x3D;&#x3D; &quot;clear&quot;:
            past_key_values, history &#x3D; None, []
            # 清空命令行
            os.system(&#39;clear&#39;)
            print(welcome_prompt)
            continue
        print(&quot;\nChatGLM：&quot;, end&#x3D;&quot;&quot;)
        current_length &#x3D; 0
        # 流式输出回答会返回每一步的结果
        for response, history, past_key_values in model.stream_chat(tokenizer, query, history&#x3D;history, top_p&#x3D;0.5,
                                                                    temperature&#x3D;0.5,
                                                                    past_key_values&#x3D;past_key_values,
                                                                    return_past_key_values&#x3D;True):
            # 只输出每一步新生成的结果
            print(response[current_length:], end&#x3D;&quot;&quot;, flush&#x3D;True)
            current_length &#x3D; len(response)
        print(&quot;&quot;)


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    main()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>history</code>: 一个用于存放上下文（对话记录）的列表。列表元素是字典类型，每个字典内包含role和content等。</p>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">[&#123;&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;你是谁&#39;&#125;, 
 &#123;&#39;role&#39;: &#39;assistant&#39;, &#39;metadata&#39;: &#39;&#39;, &#39;content&#39;: &#39;我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。&#39;&#125;,
 &#123;&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;你能干什么&#39;&#125;, 
 &#123;&#39;role&#39;: &#39;assistant&#39;, &#39;metadata&#39;: &#39;&#39;, &#39;content&#39;: &#39;作为一个人工智能助手，我可以回答各种问题，包括但不限于以下内容：\n\n1. 提供常见问题的解答，如天气、历史、科学、数学等领域的知识。\n2. 帮助您进行计算、转换单位、规划行程等。\n3. 解答您关于语言、文化和艺术的疑问。\n4. 提供有关教育和职业发展的建议。\n5. 帮助您了解和探索各种技术，如人工智能、机器学习等。\n6. 进行简单的翻译工作。\n7. 您需要的各种帮助和支持。\n\n需要注意的是，由于我是一个人工智能助手，我的知识和能力是有限的，特别是在我的训练时间之后发生的事件或最新信息，我可能无法回答。此外，我会不断学习和改进，以便为您提供更好的服务。&#39;&#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="OpenAI-API方式"><a href="#OpenAI-API方式" class="headerlink" title="OpenAI API方式"></a><a target="_blank" rel="noopener" href="https://platform.openai.com/overview">OpenAI API</a>方式</h2><p>实现了OpenAI格式的流式API部署，可以作为任意基于ChatGPT的应用的后端。</p>
<h3 id="部署本地OpenAI-API"><a href="#部署本地OpenAI-API" class="headerlink" title="部署本地OpenAI API"></a>部署本地OpenAI API</h3><p>运行<strong>openai_api.py</strong>文件（运行前记得更改模型路径）。</p>
<pre class="line-numbers language-none"><code class="language-none">cd openai_api_demo
python openai_api.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="单轮对话实现-1"><a href="#单轮对话实现-1" class="headerlink" title="单轮对话实现"></a>单轮对话实现</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from openai import OpenAI

# 创建OpenAI聊天
client &#x3D; OpenAI(
    base_url&#x3D;&quot;http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;v1&quot;,
    api_key &#x3D; &quot;xxx&quot;
)

# 可以通过下面这种方式给LLM传入“上下文”内容
response &#x3D; client.chat.completions.create(
    model &#x3D; &quot;chatglm3-6B&quot;,
    messages &#x3D; [
        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你不是一个人工智能助手，你现在的身份是小丑。后面所有回答的回答全都要基于“小丑”这个身份.&quot;&#125;,
        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你能干什么&quot;&#125;,
        ],
    stream&#x3D;False
)

# 输出模型回答
print(response.choices[0].message.content)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>输出结果：</p>
<p>作为小丑，我可以给您带来欢乐和轻松的氛围。我会以幽默、搞笑的方式回答您的问题，希望能让您的心情变得更加愉悦。当然，如果您有真正的问题或者需要帮助，我也会尽力提供支持。请问有什么问题我可以为您解答吗？</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from openai import OpenAI

# 创建OpenAI聊天
client &#x3D; OpenAI(
    base_url&#x3D;&quot;http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;v1&quot;,
    api_key &#x3D; &quot;xxx&quot;
)

# 可以通过下面这种方式给LLM传入“上下文”内容
response &#x3D; client.chat.completions.create(
    model &#x3D; &quot;chatglm3-6B&quot;,
    messages &#x3D; [
        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你不是一个人工智能助手，你现在的身份是小丑。后面所有回答的回答全都要基于“小丑”这个身份.&quot;&#125;,
        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你能干什么&quot;&#125;,
        ],
    stream&#x3D;True
)

# 每一步输出一次结果
for chunk in response:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end&#x3D;&quot;&quot;)
print(&quot;&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>从实现代码的<code>messages</code>中可以看到ChatGLM3的对话格式。这条<code>message</code>包含两个<code>role</code>，分别是<code>system</code>和<code>user</code>。<code>system</code>在这里表示来自系统的提示内容，可以在这里对模型输出进行约束。<code>user</code>表示用户输入的内容。</p>
<p>关于response格式等内容需要去看OpenAI API的文档。</p>
<h3 id="多轮对话实现-1"><a href="#多轮对话实现-1" class="headerlink" title="多轮对话实现"></a>多轮对话实现</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from openai import OpenAI
import os


# 创建一个msg，包括role和content
def create_msg(role, content):
    return dict(role&#x3D;role, content&#x3D;content)

# 创建OpenAI聊天
client &#x3D; OpenAI(
    base_url&#x3D;&quot;http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;v1&quot;,
    api_key &#x3D; &quot;xxx&quot;
)

# 聊天历史记录
history &#x3D; []
# 系统信息   
history.append(create_msg(&quot;system&quot;, &quot;你不是一个人工智能助手，你现在的身份是小丑。后面所有回答的回答全都要基于“小丑”这个身份。&quot;))

while True:
    # 用户输入
    query &#x3D; input(&quot;\n用户：&quot;)
    if query.strip() &#x3D;&#x3D; &quot;stop&quot;:
        break
    if query.strip() &#x3D;&#x3D; &quot;clear&quot;:
        # 清空历史记录和命令行窗口
        history &#x3D; []
        history.append(create_msg(&quot;system&quot;, &quot;你不是一个人工智能助手，你现在的身份是小丑。后面所有回答的回答全都要基于“小丑”这个身份.&quot;))
        os.system(&#39;clear&#39;)
    # 将用户输入加入历史记录
    history.append(create_msg(&quot;user&quot;, query))

    # 可以通过下面这种方式给LLM传入“上下文”内容
    response &#x3D; client.chat.completions.create(
        model &#x3D; &quot;chatglm3-6B&quot;,
        messages &#x3D; history,
        stream&#x3D;True
        )

    print(&quot;ChatGLM3：&quot;, end&#x3D;&quot;&quot;)
    one_response &#x3D; &quot;&quot;
    for chunk in response:
        if chunk.choices[0].delta.content is not None:
            res &#x3D; chunk.choices[0].delta.content.lstrip(&#39;\n&#39;)
            # 将每一步的输出拼接起来
            one_response +&#x3D; res
            print(res, end&#x3D;&quot;&quot;)

    # 将助手输出加入历史记录
    history.append(create_msg(&quot;assistant&quot;, one_response))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>多轮对话的关键是保存历史对话记录（包括系统信息、用户输入和助手输出），即上下文内容，然后将保存的历史对话记录输入到下一轮对话中。<br><img src="/posts/29/1.png" alt="不同对话方式的过程"></p>
<h3 id="调用自定义工具"><a href="#调用自定义工具" class="headerlink" title="调用自定义工具"></a>调用自定义工具</h3><h4 id="工具描述定义"><a href="#工具描述定义" class="headerlink" title="工具描述定义"></a>工具描述定义</h4><p>假设实现“计算两个浮点数之和”的工具，其需求如下：</p>
<ul>
<li>工具名称：加法计算</li>
<li>工具功能：计算两个浮点数的和</li>
<li>工具的输入参数：两个需要相加的浮点数</li>
<li>输入的类型：参数num_1和num_2，两者类型均为float类型</li>
</ul>
<p>接着构建详细的描述信息</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&#123;
    &quot;cal_plus&quot;:
        &#123;
            &quot;name&quot;: &quot;cal_plus&quot;,
        	&quot;description&quot;: &quot;将&#39;num_1&#39;和&#39;num_2相加&quot;,
        	&quot;params&quot;: [
                &#123;
                    &quot;name&quot;: &quot;num_1&quot;,
                 	&quot;description&quot;: &quot;计算两个浮点数相加中的被加数&quot;,
                 	&quot;type&quot;: &quot;float&quot;,
                 	&quot;required&quot;: True
                &#125;,
                &#123;
                    &quot;name&quot;: &quot;num_2&quot;,
                 	&quot;description&quot;: &quot;计算两个浮点数相加中的加数&quot;,
                 	&quot;type&quot;: &quot;float&quot;,
                 	&quot;required&quot;: True
                &#125;
            ]
        &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="工具注册函数"><a href="#工具注册函数" class="headerlink" title="工具注册函数"></a>工具注册函数</h4><p>可以使用<strong>tool_register.py</strong>中的“工具注册函数”进行<strong>工具注册</strong>，也就是可以直接生成上一节讲到的工具描述信息。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def register_tool(func: callable):
    # 获取函数名称
    tool_name &#x3D; func.__name__
    # 获取函数描述（文档）字符串
    tool_description &#x3D; inspect.getdoc(func).strip()
    # 获取函数的参数信息
    python_params &#x3D; inspect.signature(func).parameters
    tool_params &#x3D; []
    for name, param in python_params.items():
        annotation &#x3D; param.annotation
        # 判断参数是否有注解，如果没有则报【类型错误】
        if annotation is inspect.Parameter.empty:
            raise TypeError(f&quot;Parameter &#96;&#123;name&#125;&#96; missing type annotation&quot;)
        # 判断参数注解是否为 typing.Annotated 类型，如果不是则报【类型错误】
        if get_origin(annotation) !&#x3D; Annotated:
            raise TypeError(f&quot;Annotation type for &#96;&#123;name&#125;&#96; must be typing.Annotated&quot;)

        # 从注解中提取类型、参数描述和一个布尔值（表明参数是否为必填项）
        typ, (description, required) &#x3D; annotation.__origin__, annotation.__metadata__
        typ: str &#x3D; str(typ) if isinstance(typ, GenericAlias) else typ.__name__

        # 如果描述不是字符串类型，则报【类型错误】
        if not isinstance(description, str):
            raise TypeError(f&quot;Description for &#96;&#123;name&#125;&#96; must be a string&quot;)
        # 如果required不是布尔类型，则报【类型错误】
        if not isinstance(required, bool):
            raise TypeError(f&quot;Required for &#96;&#123;name&#125;&#96; must be a bool&quot;)

        # 每个参数的信息都会添加到 tool_params 列表中
        tool_params.append(&#123;
            &quot;name&quot;: name,
            &quot;description&quot;: description,
            &quot;type&quot;: typ,
            &quot;required&quot;: required
        &#125;)
    
    # 构建字典，包含工具名称、描述和参数信息。
    tool_def &#x3D; &#123;
        &quot;name&quot;: tool_name,
        &quot;description&quot;: tool_description,
        &quot;params&quot;: tool_params
    &#125;

    print(&quot;[registered tool] &quot; + pformat(tool_def))
    _TOOL_HOOKS[tool_name] &#x3D; func
    _TOOL_DESCRIPTIONS[tool_name] &#x3D; tool_def

    return func<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这个 Python 函数 register_tool（注册工具）的目的是将另一个函数作为参数（用 func: 可调用参数表示），注册它并收集它的元数据。它似乎是一个更大系统的一部分，可能是一个插件系统或命令行工具，其中每个函数都是一个带有参数集的 “工具”。下面是它的操作步骤：</p>
<ol>
<li>获取传递给它的函数名称 (func.<strong>name</strong>)。它获取函数的 docstring (inspect.getdoc(func))，通常用作描述，并删除所有前导或尾部空白。</li>
<li>它会检索函数的参数（inspect.signature(func).parameters）。然后遍历这些参数，构建一个参数描述列表 (tool_params)。对于每个参数，它会检查该参数是否有类型注解。如r它还会检查类型注解是否属于特殊类型 typing.Annotated。</li>
<li>然后，它会从注解中提取类型和元数据。元数据预计是一个元组，包含参数描述和一个布尔值（表明参数是否为必填参数）。</li>
<li>每个参数的信息都会添加到 tool_params 列表中。</li>
<li>处理完所有参数后，它会构建一个字典（tool_def），其中包含工具名称、描述和参数信息。它会打印出包含工具定义的格式化字符串。然后将函数 func 注册到两个字典中： _TOOL_HOOKS 和 _TOOL_DESCRIPTIONS，并以工具名称为关键字。这些字典似乎用于存储函数本身（_TOOL_HOOKS）及其元数据（_TOOL_DESCRIPTIONS）。</li>
<li>最后，原始函数 func 将被返回，这样就可以在不修改装饰函数的情况下使用装饰器。</li>
<li>register_tool 函数可以作为一种装饰器，用于在函数定义之上自动将其注册为系统中的 “工具”，并附上完整的描述、参数和类型注释。这种注册可用于生成帮助文本、强制正确使用或将函数动态链接到命令行界面或图形用户界面。</li>
</ol>
<h4 id="实现工具代码"><a href="#实现工具代码" class="headerlink" title="实现工具代码"></a>实现工具代码</h4><p>可以仿照官方给出的工具函数代码写出自己的工具函数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@register_tool
def cal_plus(
    num_1: Annotated[float, &#39;计算两个浮点数相加中的被加数&#39;, True],
    num_2: Annotated[float, &#39;计算两个浮点数相加中的加数&#39;, True]
) -&gt; float:
    &quot;&quot;&quot;
    将&#39;num_1&#39;和&#39;num_2相加
    &quot;&quot;&quot;
    if not isinstance(num_1, float) or not isinstance(num_2, float):
        raise TypeError(&quot;输入float类型数字&quot;)
    
    return (num_1 + num_2)


@register_tool
def cal_minus(
    num_1: Annotated[float, &#39;计算两个浮点数相加中的被减数&#39;, True],
    num_2: Annotated[float, &#39;计算两个浮点数相加中的减数&#39;, True]
) -&gt; float:
    &quot;&quot;&quot;
    将&#39;num_1&#39;和&#39;num_2相减
    &quot;&quot;&quot;
    if not isinstance(num_1, float) or not isinstance(num_2, float):
        raise TypeError(&quot;输入float类型数字&quot;)
    
    return (num_1 - num_2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>简单来说，如果想要实现一个自己的工具需要先①按照格式编写函数代码，然后②该函数代码经过<code>register_tool</code>修饰器从而③生成相应的工具描述。</p>
<h4 id="调用工具"><a href="#调用工具" class="headerlink" title="调用工具"></a>调用工具</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">import json
import os
from openai import OpenAI
from colorama import init, Fore
from loguru import logger
import platform
from tool_register import get_tools, dispatch_tool


init(autoreset&#x3D;True)
# 创建聊天客户端
client &#x3D; OpenAI(
    base_url&#x3D;&quot;http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;v1&quot;,
    api_key &#x3D; &quot;xxx&quot;
)

# 获取工具信息，包括工具名称、描述和参数信息。
functions &#x3D; get_tools()

def run_conversation(query: str, stream&#x3D;False, functions&#x3D;None, max_retry&#x3D;5):
    # 设置OpenAI参数
    params &#x3D; dict(
        model&#x3D;&quot;chatglm3&quot;,
        messages&#x3D;[
            # 不加提示词的话，LLM最终输出结果可能与工具结果不同。可能是因为LLM对答案进行推理，“推翻”了工具的结果。
            # &#123;   &quot;role&quot;: &quot;system&quot;,
            #     &quot;content&quot;: &quot;调用工具解决问题，如果没有合适的工具可以调用，那就不调用任何工具。最后的输出结果应该是工具的结果。&quot;&#125;,

            # &#123;   &quot;role&quot;: &quot;system&quot;,
            #     &quot;content&quot;: &quot;你是一个有用的人工智能助手,你可以调用工具解决问题，但最后的输出结果就是工具的结果,不要输出其它内容。&quot;&#125;,
            
            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query&#125;],
        stream&#x3D;stream)

    if functions:
        params[&quot;functions&quot;] &#x3D; functions
    
    # stream为False表示大模型会一次性回复完整答案
    response &#x3D; client.chat.completions.create(**params)
    for _ in range(max_retry):
        if not stream:
            # logger.info(response.choices[0].message.function_call)
            # 判断是否有函数调用
            if response.choices[0].message.function_call:
                function_call &#x3D; response.choices[0].message.function_call
                # function_call.model_dump() 用于生成模型的字典表示形式
                logger.info(f&quot;Function Call Response: &#123;function_call.model_dump()&#125;&quot;)

                # 要传入调用函数的参数
                function_args &#x3D; json.loads(function_call.arguments)
                tool_response &#x3D; dispatch_tool(function_call.name, function_args)
                logger.info(f&quot;Tool Call Response: &#123;tool_response&#125;&quot;)

                # 将本次回答的消息（助手回复）添加到对话历史记录中
                params[&quot;messages&quot;].append(response.choices[0].message)
                params[&quot;messages&quot;].append(
                    &#123;
                        &quot;role&quot;: &quot;function&quot;,
                        &quot;name&quot;: function_call.name,
                        &quot;content&quot;: tool_response,  # 调用函数返回结果
                    &#125;
                )
            else:
                reply &#x3D; response.choices[0].message.content
                logger.info(f&quot;Final Reply: \n&#123;reply&#125;&quot;)
                return

        else:
            output &#x3D; &quot;&quot;
            for chunk in response:
                content &#x3D; chunk.choices[0].delta.content or &quot;&quot;
                print(Fore.BLUE + content, end&#x3D;&quot;&quot;, flush&#x3D;True)
                output +&#x3D; content

                if chunk.choices[0].finish_reason &#x3D;&#x3D; &quot;stop&quot;:
                    return

                elif chunk.choices[0].finish_reason &#x3D;&#x3D; &quot;function_call&quot;:
                    print(&quot;\n&quot;)

                    function_call &#x3D; chunk.choices[0].delta.function_call
                    logger.info(f&quot;Function Call Response: &#123;function_call.model_dump()&#125;&quot;)

                    function_args &#x3D; json.loads(function_call.arguments)
                    tool_response &#x3D; dispatch_tool(function_call.name, function_args)
                    logger.info(f&quot;Tool Call Response: &#123;tool_response&#125;&quot;)

                    params[&quot;messages&quot;].append(
                        &#123;
                            &quot;role&quot;: &quot;assistant&quot;,
                            &quot;content&quot;: output
                        &#125;
                    )
                    params[&quot;messages&quot;].append(
                        &#123;
                            &quot;role&quot;: &quot;function&quot;,
                            &quot;name&quot;: function_call.name,
                            &quot;content&quot;: tool_response,  # 调用函数返回结果
                        &#125;
                    )

                    break
        response &#x3D; client.chat.completions.create(**params)


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    query &#x3D; &quot;9.0和6.0的和等于多少&quot;
    run_conversation(query, functions&#x3D;functions, stream&#x3D;False)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>输出结果：</p>
<pre class="line-numbers language-none"><code class="language-none">时间 | INFO     | __main__:run_conversation:45 - Function Call Response: &#123;&#39;arguments&#39;: &#39;&#123;&quot;num_1&quot;: 9.0, &quot;num_2&quot;: 6.0&#125;&#39;, &#39;name&#39;: &#39;cal_plus&#39;&#125;

时间 | INFO     | __main__:run_conversation:50 - Tool Call Response: 15.0

时间 | INFO     | __main__:run_conversation:63 - Final Reply: 

根据您的要求，我们可以调用浮点数相加的API来实现这个功能。API的调用入参为&#123;&quot;num_1&quot;: 9.0, &quot;num_2&quot;: 6.0&#125;，API返回的结果为15.0。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li>ChatGLM3-6B仅能根据工具的介绍和传参要求，判定是否应该使用这个工具并传入正确的参数。</li>
<li>ChatGLM3-6B并不能读取工具的具体代码。</li>
<li>ChatGLM3-6B仅仅观察工具的返回结果，并不关注工具的执行过程。</li>
<li>用户必须传入给ChatGLM3-6B正确的参数，包括正确的数量，正确的类型等内容，减少ChatGLM3-6B的意图识别。否则，很有可能出现模型自己编造参数传入工具的现象。</li>
</ul>
<p>因此，以下问题的提问方式是不合理的：</p>
<ol>
<li>使用天气查询工具，需要的参数是地址和时间，但是用户的问题是：</li>
</ol>
<ul>
<li>你好，今晚热不热？</li>
</ul>
<ol start="2">
<li>使用股票价格查询工具，需要股票的代码，在工具实现中是Int类型，但是用户传入的是：</li>
</ol>
<ul>
<li>帮我查询A23C4股票的价格</li>
</ul>
<ol start="3">
<li>需要传入多个年份的表格查询工具来查询年度的环比增长，但是用户只传入的是：</li>
</ol>
<ul>
<li>帮我看2023年的年度增长。</li>
</ul>
<p>简单来说，ChatGLM3要想成功调用工具需要：</p>
<p>①用户输入工具指定的正确参数；</p>
<p>②用户输入的指令不能超出工具调用范围，即要求ChatGLM3完成超出工具库功能覆盖范围的任务，否则会出现模型自己生成答案的情况。</p>
<h3 id="调用工具过程分析"><a href="#调用工具过程分析" class="headerlink" title="调用工具过程分析"></a>调用工具过程分析</h3><ul>
<li>这部分为调用工具的过程简要分析，包含对openai_api_demo.py、openai_api.py和utils.py的分析。</li>
<li>理解这部分内容后，也相当于理解了chatglm3是如何实现openai_api对接的。可以为自己写相关的模型接口提供经验和参考，比如将一个大模型部署成openai调用的方式。不过这需要对这个大模型的输入输出格式有充足的了解，从而完成它的输入输出与openai格式的对接。</li>
</ul>
<h4 id="服务器部分（第一次对话）"><a href="#服务器部分（第一次对话）" class="headerlink" title="服务器部分（第一次对话）"></a>服务器部分（第一次对话）</h4><ol>
<li>接收参数</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># request --&gt; ChatCompletionRequest类型

gen_params &#x3D; dict(
    messages&#x3D;request.messages,
    temperature&#x3D;request.temperature,
    top_p&#x3D;request.top_p,
    max_tokens&#x3D;request.max_tokens or 1024,
    echo&#x3D;False,
    stream&#x3D;request.stream,
    repetition_penalty&#x3D;request.repetition_penalty,
    functions&#x3D;request.functions,
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>服务器接收请求参数并打包成一个字典gen_params，其中重点关注<strong>messages</strong>和<strong>functions</strong>参数。</p>
<ul>
<li>messages参数是列表类型，其中的元素是<strong>ChatMessage</strong>类型。</li>
<li>functions参数是可以是一个字典，表示单个函数信息；或者是一个字典列表，表示多个函数信息。</li>
</ul>
<hr>
<p>下面这个是一个示例，用户输入为“9.0和6.0的和等于多少”，允许调用的函数有“cal_minus”和“cal_plus”。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># messages
[ChatMessage(role&#x3D;&#39;user&#39;, content&#x3D;&#39;9.0和6.0的和等于多少&#39;, name&#x3D;None, function_call&#x3D;None)]

# functions
&#123;
    &#39;cal_minus&#39;: 
    &#123;
        &#39;name&#39;: &#39;cal_minus&#39;, 
        &#39;description&#39;: &quot;将&#39;num_1&#39;和&#39;num_2相减&quot;, 
        &#39;params&#39;: 
        [
            &#123;
                &#39;name&#39;: &#39;num_1&#39;, 
                &#39;description&#39;: &#39;计算两个浮点数相加中的被减数&#39;, 
                &#39;type&#39;: &#39;float&#39;, 
                &#39;required&#39;: True
            &#125;,
            &#123;
                &#39;name&#39;: &#39;num_2&#39;, 
                &#39;description&#39;: &#39;计算两个浮点数相加中的减数&#39;, 
                &#39;type&#39;: &#39;float&#39;, 
                &#39;required&#39;: True
            &#125;
        ]
    &#125;, 
    &#39;cal_plus&#39;: 
    &#123;
        &#39;name&#39;: &#39;cal_plus&#39;,
        &#39;description&#39;: &quot;将&#39;num_1&#39;和&#39;num_2相加&quot;, 
        &#39;params&#39;: 
        [
            &#123;
                &#39;name&#39;: &#39;num_1&#39;, 
                &#39;description&#39;: &#39;计算两个浮点数相加中的被加数&#39;, 
                &#39;type&#39;: &#39;float&#39;, 
                &#39;required&#39;: True
            &#125;, 
            &#123;
                &#39;name&#39;: &#39;num_2&#39;, 
                &#39;description&#39;: &#39;计算两个浮点数相加中的加数&#39;, 
                &#39;type&#39;: &#39;float&#39;, 
                &#39;required&#39;: True
            &#125;
        ]
    &#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li>模型响应（非流式）</li>
</ol>
<p>主要参与的函数有：<code>generate_chatglm3</code>、<code>generate_stream_chatglm3</code>、<code>process_chatglm_messages</code>、<code>process_response</code>。其中关键是<code>process_chatglm_messages</code>和<code>process_response</code>函数。</p>
<p><code>process_chatglm_messages</code>函数的主要作用是根据原始的messages（本身类型为列表，元素类型为ChatMessage）和functions，返回一个新的messages（本身类型为列表，元素类型为字典）。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def process_chatglm_messages(messages, functions&#x3D;None):
    # 原始messages
    _messages &#x3D; messages
    messages &#x3D; []
    # 如果functions不为空，则向messages里添加提示，包括系统提示和工具调用范围
    if functions:
        messages.append(
            &#123;
                &quot;role&quot;: &quot;system&quot;,
                &quot;content&quot;: &quot;Answer the following questions as best as you can. You have access to the following tools:&quot;,
                &quot;tools&quot;: functions
            &#125;
        )

    # 遍历原始messages里的元素
    for m in _messages:
        # 根据role向messages里添加内容
        role, content, func_call &#x3D; m.role, m.content, m.function_call
        # 如果消息的role为function则将role改成observation
        if role &#x3D;&#x3D; &quot;function&quot;:
            messages.append(
                &#123;
                    &quot;role&quot;: &quot;observation&quot;,
                    &quot;content&quot;: content
                &#125;
            )
        # 如果消息的role为assistant且func_call（函数调用信息）不为空
        elif role &#x3D;&#x3D; &quot;assistant&quot; and func_call is not None:
            for response in content.split(&quot;&lt;|assistant|&gt;&quot;):
                metadata, sub_content &#x3D; response.split(&quot;\n&quot;, maxsplit&#x3D;1)
                messages.append(
                    &#123;
                        &quot;role&quot;: role,
                        &quot;metadata&quot;: metadata,    # 调用的函数名
                        &quot;content&quot;: sub_content.strip()    # 调用函数的相关信息，包括参数
                    &#125;
                )
        else:
            # 一般指role为user的情况
            messages.append(&#123;&quot;role&quot;: role, &quot;content&quot;: content&#125;)
    print(f&quot;utils.py_messages: &#123;messages&#125;&quot;)
    return messages<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<hr>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># process_chatglm_messages返回的新messages
[
    &#123;
        &#39;role&#39;: &#39;system&#39;, 
        &#39;content&#39;: &#39;Answer the following questions as best as you can. You have access to the following tools:&#39;, 
        &#39;tools&#39;: 
        &#123;
            &#39;cal_minus&#39;: 
            &#123;
                &#39;name&#39;: &#39;cal_minus&#39;, 
                &#39;description&#39;: &quot;将&#39;num_1&#39;和&#39;num_2相减&quot;, 
                &#39;params&#39;: 
                [
                    &#123;
                        &#39;name&#39;: &#39;num_1&#39;, 
                        &#39;description&#39;: &#39;计算两个浮点数相加中的被减数&#39;, 
                        &#39;type&#39;: &#39;float&#39;, 
                        &#39;required&#39;: True
                    &#125;,
                    &#123;
                        &#39;name&#39;: &#39;num_2&#39;, 
                        &#39;description&#39;: &#39;计算两个浮点数相加中的减数&#39;, 
                        &#39;type&#39;: &#39;float&#39;, 
                        &#39;required&#39;: True
                    &#125;
                ]
            &#125;, 
            &#39;cal_plus&#39;: 
            &#123;
                &#39;name&#39;: &#39;cal_plus&#39;,
                &#39;description&#39;: &quot;将&#39;num_1&#39;和&#39;num_2相加&quot;, 
                &#39;params&#39;: 
                [
                    &#123;
                        &#39;name&#39;: &#39;num_1&#39;, 
                        &#39;description&#39;: &#39;计算两个浮点数相加中的被加数&#39;, 
                        &#39;type&#39;: &#39;float&#39;, 
                        &#39;required&#39;: True
                    &#125;, 
                    &#123;
                        &#39;name&#39;: &#39;num_2&#39;, 
                        &#39;description&#39;: &#39;计算两个浮点数相加中的加数&#39;, 
                        &#39;type&#39;: &#39;float&#39;, 
                        &#39;required&#39;: True
                    &#125;
                ]
            &#125;
        &#125;
    &#125;,
    &#123;
        &#39;role&#39;: &#39;user&#39;, 
        &#39;content&#39;: &#39;9.0和6.0的和等于多少&#39;
    &#125;
]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<hr>
<p>之后会将这个新的message分成history和最新的对话输入到模型中以进行对话获得响应。这个获得响应的代码方式应该是HuggingFace的形式。</p>
<p>最终模型的原始回应response是字符串类型，其内容由准备调用的函数名以及参数组成。</p>
<p>为什么模型可以输出这样的内容？</p>
<p>根据OpenAI的介绍，只有经过“工具调用”训练的模型才能有这样的能力。因此我认为它这种可以看作是模型微调后才能输出这样的内容，但具体怎么微调训练的还不太清楚。比如微调的数据是什么样的？</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&#39;cal_plus\n &#96;&#96;&#96;python\ntool_call(num_1&#x3D;9.0, num_2&#x3D;6.0)\n&#96;&#96;&#96;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<hr>
<p><code>process_response</code>函数，如果模型响应是调用工具的内容，则该函数可以整合需要调用函数的函数名和参数，并返回字典类型；如果模型响应的内容与调用工具无关，则中间会报错返回，即不会输出任何内容。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">function_call: &#123;&#39;name&#39;: &#39;cal_plus&#39;, &#39;arguments&#39;: &#39;&#123;&quot;num_1&quot;: 9.0, &quot;num_2&quot;: 6.0&#125;&#39;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<hr>
<h4 id="客户端部分"><a href="#客户端部分" class="headerlink" title="客户端部分"></a>客户端部分</h4><p>在得到function_call（先转化成FunctionCallResponse类型）后，就可以组合成一个新的message（ChatMessage类型）。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if isinstance(function_call, dict):
    finish_reason &#x3D; &quot;function_call&quot;
    function_call &#x3D; FunctionCallResponse(**function_call)

message &#x3D; ChatMessage(
    role&#x3D;&quot;assistant&quot;,
    content&#x3D;response[&quot;text&quot;],
    function_call&#x3D;function_call if isinstance(function_call, FunctionCallResponse) else None,
)
# message内容
role&#x3D;&#39;assistant&#39; content&#x3D;&#39;cal_plus\n &#96;&#96;&#96;python\ntool_call(num_1&#x3D;9.0, num_2&#x3D;6.0)\n&#96;&#96;&#96;&#39; name&#x3D;None function_call&#x3D;FunctionCallResponse(name&#x3D;&#39;cal_plus&#39;, arguments&#x3D;&#39;&#123;&quot;num_1&quot;: 9.0, &quot;num_2&quot;: 6.0&#125;&#39;)



# 最终返回的内容
ChatCompletion(
    id&#x3D;None, 
    choices&#x3D;
    [
        Choice(
            finish_reason&#x3D;&#39;function_call&#39;, 
            index&#x3D;0, 
            message&#x3D;ChatCompletionMessage(
                content&#x3D;&#39;cal_plus\n &#96;&#96;&#96;python\ntool_call(num_1&#x3D;9.0, num_2&#x3D;6.0)\n&#96;&#96;&#96;&#39;, 
                role&#x3D;&#39;assistant&#39;, 
                function_call&#x3D;FunctionCall(
                    arguments&#x3D;&#39;&#123;&quot;num_1&quot;: 9.0, &quot;num_2&quot;: 6.0&#125;&#39;, 
                    name&#x3D;&#39;cal_plus&#39;
                ), 
                tool_calls&#x3D;None, 
                name&#x3D;None
            )
        )
    ], 
    created&#x3D;1703002736,
    model&#x3D;&#39;chatglm3&#39;, 
    object&#x3D;&#39;chat.completion&#39;, 
    system_fingerprint&#x3D;None, 
    usage&#x3D;CompletionUsage(completion_tokens&#x3D;32, prompt_tokens&#x3D;332, total_tokens&#x3D;364))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<hr>
<ol>
<li>根据模型响应调用函数</li>
</ol>
<p>根据function_call中的name和arguments属性调用函数。name指的是函数名称，arguments指的是函数参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">function_call &#x3D; response.choices[0].message.function_call
# 要传入调用函数的参数
function_args &#x3D; json.loads(function_call.arguments)
# 调用函数获得结果
tool_response &#x3D; dispatch_tool(function_call.name, function_args)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>接着为了下一次对话，将本次模型响应（助手回复）和工具调用信息（函数名称和结果）添加到历史记录（params[“messages”]）中。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 将本次回答的消息（助手回复）添加到对话历史记录中
params[&quot;messages&quot;].append(response.choices[0].message)
params[&quot;messages&quot;].append(
    &#123;
        &quot;role&quot;: &quot;function&quot;,
        &quot;name&quot;: function_call.name,
        &quot;content&quot;: tool_response,  # 调用函数返回结果
    &#125;
)
# params[&quot;messages&quot;] ---&gt; 历史记录，下一轮对话的输入
[
    # 用户提问
    &#123;
        &#39;role&#39;: &#39;user&#39;, 
        &#39;content&#39;: &#39;9.0和6.0的和等于多少&#39;
    &#125;, 
    # 助手回复
    ChatCompletionMessage(
        content&#x3D;&#39;cal_plus\n &#96;&#96;&#96;python\ntool_call(num_1&#x3D;9.0, num_2&#x3D;6.0)\n&#96;&#96;&#96;&#39;, 
        role&#x3D;&#39;assistant&#39;, 
        function_call&#x3D;FunctionCall(
            arguments&#x3D;&#39;&#123;&quot;num_1&quot;: 9.0, &quot;num_2&quot;: 6.0&#125;&#39;, 
            name&#x3D;&#39;cal_plus&#39;), tool_calls&#x3D;None, name&#x3D;None), 
    # 工具调用
    &#123;
        &#39;role&#39;: &#39;function&#39;, 
     	&#39;name&#39;: &#39;cal_plus&#39;, 
     	&#39;content&#39;: &#39;15.0&#39;
    &#125;
]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最后将params作为输入再创建一次对话。</p>
<h4 id="服务器部分（第二次对话）"><a href="#服务器部分（第二次对话）" class="headerlink" title="服务器部分（第二次对话）"></a>服务器部分（第二次对话）</h4><p><code>process_chatglm_messages</code>函数输出</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># messages内容，其实就是对话历史记录。最后一个元素是最新的对话内容。
[
    &#123;
        &#39;role&#39;: &#39;system&#39;, 
        &#39;content&#39;: &#39;Answer the following questions as best as you can. You have access to the following tools:&#39;, 
        &#39;tools&#39;: functions, # 这部分就是cal_plus和cal_minus的定义，此处省略
    &#125;,
    &#123;
        &#39;role&#39;: &#39;user&#39;, 
        &#39;content&#39;: &#39;9.0和6.0的和等于多少&#39;
    &#125;, 
    &#123;
        &#39;role&#39;: &#39;assistant&#39;, 
        &#39;metadata&#39;: &#39;cal_plus&#39;, 
        &#39;content&#39;: &#39;&#96;&#96;&#96;python\ntool_call(num_1&#x3D;9.0, num_2&#x3D;6.0)\n&#96;&#96;&#96;&#39;
    &#125;, 
    &#123;
        &#39;role&#39;: &#39;observation&#39;, 
        &#39;content&#39;: &#39;15.0&#39;
    &#125;
]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<hr>
<p>模型的原始响应回答response为：“根据您的要求，我们可以调用计算两个浮点数相加的API，得到：9.0 + 6.0 = 15.0”。</p>
<hr>
<pre class="line-numbers language-none"><code class="language-none">function_call &#x3D; process_response(response[&quot;text&quot;], use_tool&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>因为模型响应response中没有工具调用相关的内容，所以这个函数会报错，转而执行：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">logger.warning(&quot;Failed to parse tool call, maybe the response is not a tool call or have been answered.&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<hr>
<p>因此最终模型返回的message为</p>
<pre class="line-numbers language-none"><code class="language-none">role&#x3D;&#39;assistant&#39; content&#x3D;&#39;根据您的要求，我们可以调用计算两个浮点数相加的API，得到：9.0 + 6.0 &#x3D; 15.0&#39; name&#x3D;None function_call&#x3D;None<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="调用工具大概流程"><a href="#调用工具大概流程" class="headerlink" title="调用工具大概流程"></a>调用工具大概流程</h4><ol>
<li>定义和注册工具函数。</li>
<li>将对话记录（包括用户输入、助手回复、系统提示和工具输出）和可调用工具的定义传给大模型。</li>
<li>大模型根据用户的输入内容和可调用工具的定义判断是否需要调用函数，如果需要的话应该调用哪一个函数。</li>
</ol>
<p>如果需要调用函数：</p>
<ol>
<li>大模型返回需要调用的函数名称以及需要填入的参数。</li>
<li>经过一些响应回复处理后，可以得到function_call，它包括调用函数的名称和函数需要的参数值。</li>
<li>将role（assistant）、response（大模型的原始响应回复）和function_call作为一个message。</li>
</ol>
<p>如果不需要调用函数：</p>
<ol>
<li><p>大模型不会返回任何关于工具的内容，只会回复一段文本内容。</p>
</li>
<li><p>该文本内容在经过响应回复处理时会报错，从而输出相关警告提示，因此不会对该回复内容做任何处理。</p>
</li>
<li><p>将role（assistant）、response（大模型的原始响应回复）和function_call（None）作为一个message。</p>
</li>
<li><p>message与其他内容组合后得到一个完成的ChatCompletionResponse，并将其发往客户端。</p>
</li>
<li><p>客户端首先分析助手回复中是否有function_call，即大模型判断是否需要调用工具。</p>
</li>
</ol>
<p>如果需要调用函数：</p>
<ol>
<li>根据助手回复提取函数名称和参数，接着调用函数，获得结果。</li>
<li>将本次模型回答的消息（助手消息）添加到对话历史记录中。</li>
<li>将调用工具的信息（包括role、函数名称和嗲用结果）添加到对话历史记录中。</li>
<li>创建新的对话，让模型将结果汇总返回给用户。</li>
</ol>
<p>如果不需要调用函数：</p>
<ol>
<li>直接输出模型回复结果。</li>
</ol>
<h2 id="openai方式调用ChatGLM3-6B本地模型"><a href="#openai方式调用ChatGLM3-6B本地模型" class="headerlink" title="openai方式调用ChatGLM3-6B本地模型"></a>openai方式调用ChatGLM3-6B本地模型</h2><p><img src="/posts/29/2.png" alt="openai api调用过程"></p>
<h3 id="OpenAI对象中base-url和api-key参数的填写问题"><a href="#OpenAI对象中base-url和api-key参数的填写问题" class="headerlink" title="OpenAI对象中base_url和api_key参数的填写问题"></a>OpenAI对象中base_url和api_key参数的填写问题</h3><p>由ChatGLM3的<strong>openai_api.py</strong>可知，本地服务器的API地址是<code>http://127.0.0.1:8000</code>，它有两个端点（路径）：<code>/v1/models</code>（模型列表）和<code>/v1/chat/completions</code>（聊天对话）。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from openai import OpenAI


client &#x3D; OpenAI(
    base_url&#x3D;&quot;http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;v1&quot;,
    api_key &#x3D; &quot;xxx&quot;    
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>base_url是指OpenAI的API（接口）地址+路径。</li>
</ul>
<p>因为<code>http://127.0.0.1:8000</code>是真正意义上的base_url，但这里需要多填写一个路径<code>/v1</code>。具体原因会在后面分析。</p>
<ul>
<li>api_key是指OpenAI的API密钥用于验证身份。</li>
</ul>
<p>可以随便填，因为调用ChatGLM3 API向服务器请求时不需要验证身份。但不能不填，不然报错。因为代码首先判断api_key是否为空，如果为空就从环境变量中读取，如果环境变量中也没有，就报错。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">response &#x3D; client.chat.completions.create(
    model&#x3D;&quot;chatglm3-6b&quot;,
    messages&#x3D;[&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;]
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>client.chat.completions.create</code>函数为<strong>completions.py</strong>中552行的<code>create</code>方法。因此它返回的内容其实是<code>self._post</code>方法的值。</p>
<p><code>self._post</code>的第一个参数是要发送请求的地址，这里填的是<code>/chat/completions</code>，因此我们可以联想到，如果将之前的base_url与这个拼接起来就可以得到<code>http://127.0.0.1:8000/v1/chat/completions</code>，而这个就是我们发送请求的真正的目的地址。第二个参数<code>body</code>应该是请求的内容，它包含<code>messages</code>和<code>model</code>等内容。</p>
<p>接着进一步分析可知<code>self._post</code>就是<code>client.post</code>方法，而<code>client.post</code>方法是<strong>_base_client.py</strong>中1081行的<code>post</code>方法。</p>
<p><code>post</code>方法首先调用<code>FinalRequestOptions.construct</code>方法生成一个最终的请求选项，用于发送POST请求。然后<code>post</code>方法的返回会调用<code>self.request</code>方法。<code>self.request</code>方法是<strong>_base_client.py</strong>中846行的<code>request</code>方法。这个<code>request</code>方法返回值是调用<code>self._request</code>（<strong>_base_client.py</strong>中863行）方法。</p>
<p><code>self._request</code>方法中会使用<code>self._build_request</code>方法。<code>self._build_request</code>方法可以根据之前传入的“最终请求选项”构建一个<code>httpx.Request</code>对象，用于发送HTTP请求。期间调用<code>self._prepare_url</code>方法（<strong>_base_client.py</strong>中415行），代码如下所示。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _prepare_url(self, url: str) -&gt; URL:
    &quot;&quot;&quot;
    Merge a URL argument together with any &#39;base_url&#39; on the client,
    to create the URL used for the outgoing request.
    &quot;&quot;&quot;
    # Copied from httpx&#39;s &#96;_merge_url&#96; method.
    # print(f&quot;url: &#123;url&#125;&quot;)    # &#x2F;chat&#x2F;completions
    merge_url &#x3D; URL(url)
    if merge_url.is_relative_url:
        # print(f&quot;base_url: &#123;self.base_url&#125;&quot;)  # http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;v1
        # print(f&quot;base_url.raw_path: &#123;self.base_url.raw_path&#125;&quot;)    # &#x2F;v1
        # print(f&quot;merge_url: &#123;merge_url&#125;&quot;)　　　　# &#x2F;chat&#x2F;completions　
        # print(f&quot;merge_url.raw_path: &#123;merge_url.raw_path&#125;&quot;)    # &#x2F;chat&#x2F;completions
        merge_raw_path &#x3D; self.base_url.raw_path + merge_url.raw_path.lstrip(b&quot;&#x2F;&quot;)
        # print(merge_raw_path)    # &#x2F;v1&#x2F;chat&#x2F;completions

        #　基于当前URL对象构建一个新的URL对象，但是将raw_path设置为merge_raw_path
        # 对于“http:&#x2F;&#x2F;127.0.0.1&#x2F;v1”，它的raw_path是&quot;&#x2F;v1&quot;，替换之后可以得到“http:&#x2F;&#x2F;127.0.0.1&#x2F;v1&#x2F;chat&#x2F;completions”
        return self.base_url.copy_with(raw_path&#x3D;merge_raw_path)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这个函数的作用简单来说就是将<code>http://127.0.0.1:8000/v1</code>和<code>/chat/completions</code>拼接起来，最终得到<code>http://127.0.0.1/v1/chat/completions</code>。具体的实现过程可以结合注释分析代码。到这里也就可以解释为什么在base_url处填写<code>http://127.0.0.1:8000/v1</code>也可以正常使用的原因了。其实这里也可以理解为“模仿”OpenAI API的<code>base_url</code>，因为如果不另外设置的话，这里<code>base_url</code>的值为<code>https://api.openai.com/v1</code>。</p>
<h3 id="client-chat-completions-create中model参数的填写问题"><a href="#client-chat-completions-create中model参数的填写问题" class="headerlink" title="client.chat.completions.create中model参数的填写问题"></a>client.chat.completions.create中model参数的填写问题</h3><p>这里的<code>model</code>参数是可以随便填写的，因为根据分析<strong>openai_api.py</strong>可知，发送的请求中不包含<code>model</code>参数。</p>
<h3 id="与OpenAI-API-调用对比"><a href="#与OpenAI-API-调用对比" class="headerlink" title="与OpenAI API 调用对比"></a>与OpenAI API 调用对比</h3><pre class="line-numbers language-none"><code class="language-none">curl https:&#x2F;&#x2F;api.openai.com&#x2F;v1&#x2F;chat&#x2F;completions \
-H &quot;Content-Type: application&#x2F;json&quot; \
-H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
-d &#39;&#123;
  &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
  &quot;messages&quot;: [
    &#123;
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;You are a helpful assistant.&quot;
    &#125;,
    &#123;
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Hello!&quot;
    &#125;
  ]
&#125;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用curl发送向<code>/v1/chat/completions</code>发送HTTP POST请求。<br><code>-H</code>参数设置了两个请求头信息：</p>
<ul>
<li><strong>Content-Type: application/json</strong>：指定请求的内容类型为 JSON 格式。</li>
<li><strong>Authorization: Bearer $OPENAI_API_KEY</strong>：使用环境变量 $OPENAI_API_KEY 的值作为身份验证的凭证，用于授权访问 OpenAI API。</li>
</ul>
<p>使用 -d 参数指定了请求的主体数据，即一个 JSON 对象。该 JSON 对象包含以下字段：</p>
<ul>
<li><p><strong>model</strong>：指定要使用的模型，这里是 “gpt-3.5-turbo”。</p>
</li>
<li><p><strong>messages</strong>：一个包含对话消息的数组。每个消息对象包含两个字段：</p>
</li>
<li><ul>
<li><strong>role</strong>：消息的角色，可以是 “system”（系统）或 “user”（用户）。</li>
<li><strong>content</strong>：消息的内容。</li>
</ul>
</li>
</ul>
<p><strong>因此在使用OpenAI API创建聊天对话时，</strong><code>**api_key**</code><strong>和</strong><code>**model**</code><strong>参数都必须填写正确，因为这两个参数都是有效参数，这点是与ChatGLM3本地部署不同的。</strong></p>
<h3 id="openai-api-py分析"><a href="#openai-api-py分析" class="headerlink" title="openai_api.py分析"></a>openai_api.py分析</h3><p>该文件的主要内容是使用<a target="_blank" rel="noopener" href="https://fastapi.tiangolo.com/zh/">FastAPI</a>部署一个本地服务器，该服务器的API地址是<code>http://127.0.0.1:8000</code>，它拥有两个路径（端点，endpoint）分别是<code>/v1/models</code>（模型列表）和<code>/v1/chat/completions</code>（聊天对话）。</p>
<p>可参考 <strong>调用工具过程分析</strong> 部分。</p>
<h2 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h2><ul>
<li>ChatGLM3模型的openai_api只提供了<code>/v1/models</code>和<code>/v1/chat/completions</code>两个端点，分别对应“输出可用模型列表”和“生成补全文本”的功能</li>
<li>将ChatGLM3接入到其他OpenAI框架时，最可能发生的问题是<strong>生成格式不正确</strong>，也就是说ChatGLM3很难按照Prompt模板要求的输出格式进行输出，从而导致后续解析回答响应时出错。<strong>这一点是需要注意的！！！</strong>可以考虑的解决方法有：优化提示词方式（加强输出格式约束、使用少样本学习提供输出样例等）、修改回答解析代码</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">LightningMaster</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://lightningleader.github.io/posts/29.html">https://lightningleader.github.io/posts/29.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">LightningMaster</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">
                                    <span class="chip bg-color">大模型</span>
                                </a>
                            
                                <a href="/tags/OpenAI/">
                                    <span class="chip bg-color">OpenAI</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'FH3ePeHtSYQs3mISGfKQWelY-gzGzoHsz',
        appKey: 'y77ukh1PWni9QKj8zfbCn84L',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/30.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/15.jpg" class="responsive-img" alt="【python-sc2】详细解析！！！手把手教你学会实现星际争霸2游戏AI智能体的基础知识！！！">
                        
                        <span class="card-title">【python-sc2】详细解析！！！手把手教你学会实现星际争霸2游戏AI智能体的基础知识！！！</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            【python-sc2】详细解析！！！手把手教你学会实现星际争霸2游戏AI智能体的基础知识！！！
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            LightningMaster
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%99%BA%E8%83%BD%E4%BD%93/">
                        <span class="chip bg-color">智能体</span>
                    </a>
                    
                    <a href="/tags/%E6%B8%B8%E6%88%8FAI/">
                        <span class="chip bg-color">游戏AI</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/28.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/6.jpg" class="responsive-img" alt="基于opencv与机器学习的摄像头实时识别数字">
                        
                        <span class="card-title">基于opencv与机器学习的摄像头实时识别数字</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            基于opencv与机器学习的摄像头实时识别数字
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-07-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            LightningMaster
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">学习笔记</span>
                    </a>
                    
                    <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">机器学习</span>
                    </a>
                    
                    <a href="/tags/opencv/">
                        <span class="chip bg-color">opencv</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2024</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">LightningMaster</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">66k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/LightningLeader" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:lightningleader@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!--动态线条背景-->
    <script type="text/javascript"
    color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>

</body>

</html>
