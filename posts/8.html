<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="机器学习入门笔记, LightningMaster">
    <meta name="description" content="老废物">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>机器学习入门笔记 | LightningMaster</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">LightningMaster</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">LightningMaster</div>
        <div class="logo-desc">
            
            老废物
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">机器学习入门笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">学习笔记</span>
                            </a>
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">机器学习</span>
                            </a>
                        
                            <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                                <span class="chip bg-color">人工智能</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-02-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-02-28
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.9k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            <div class="info-break-policy">
                
                <i class="fa fa-pencil"></i> 作者: LightningMaster
                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="机器学习入门笔记"><a href="#机器学习入门笔记" class="headerlink" title="机器学习入门笔记"></a>机器学习入门笔记</h1><a id="more"></a>

<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个是根据“黑马程序员”的一套机器学习课程编写的博客。这篇博客上的案例均为课程里面的，但并没有包含所有案例，只是记录了部分。</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nt411r7tj">B站视频原地址</a></p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="Tf-idf文本特征提取"><a href="#Tf-idf文本特征提取" class="headerlink" title="Tf-idf文本特征提取"></a>Tf-idf文本特征提取</h3><h4 id="Tf-idf介绍"><a href="#Tf-idf介绍" class="headerlink" title="Tf-idf介绍"></a>Tf-idf介绍</h4><ul>
<li><p>TF-IDF的主要思想是：如果<strong>某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现</strong>，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p>
</li>
<li><p><strong>TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</strong></p>
</li>
<li><p>词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率</p>
</li>
<li><p>逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以<strong>由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到</strong></p>
</li>
</ul>
<p><img src="/posts/8/1.png" alt="tfidf公式"></p>
<p>最终得出结果可以理解为重要程度。</p>
<p>注：假如一篇文件的总词语数是100个，而词语”非常”出现了5次，那么”非常”一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现”非常”一词的文件数。所以，如果”非常”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后”非常”对于这篇文档的tf-idf的分数为0.05 * 3=0.15</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer    # Tf-idf文本特征提取
import jieba    # 分词模块

def cut_word(text):
    &quot;&quot;&quot;
    对中文进行分词
    &quot;我爱北京天安门&quot;————&gt;&quot;我 爱 北京 天安门&quot;
    :param text:
    :return: text
    &quot;&quot;&quot;
    # 用结巴对中文字符串进行分词
    text &#x3D; &quot; &quot;.join(list(jieba.cut(text)))

    return text

def text_chinese_tfidf_demo():
    &quot;&quot;&quot;
    对中文进行特征抽取
    :return: None
    &quot;&quot;&quot;
    data &#x3D; [&quot;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。&quot;,
            &quot;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。&quot;,
            &quot;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。&quot;]
    # 将原始数据转换成分好词的形式
    text_list &#x3D; []
    for sent in data:
        text_list.append(cut_word(sent))
    print(text_list)

    # 1、实例化一个转换器类
    # transfer &#x3D; CountVectorizer(sparse&#x3D;False)
    transfer &#x3D; TfidfVectorizer(stop_words&#x3D;[&#39;一种&#39;, &#39;不会&#39;, &#39;不要&#39;])
    # 2、调用fit_transform
    data &#x3D; transfer.fit_transform(text_list)
    print(&quot;文本特征抽取的结果：\n&quot;, data.toarray())
    print(&quot;返回特征名字：\n&quot;, transfer.get_feature_names())

    return None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>返回结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">Building prefix dict from the default dictionary ...
Loading model from cache &#x2F;var&#x2F;folders&#x2F;mz&#x2F;tzf2l3sx4rgg6qpglfb035_r0000gn&#x2F;T&#x2F;jieba.cache
Loading model cost 0.856 seconds.
Prefix dict has been built succesfully.
[&#39;一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。&#39;, &#39;我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。&#39;, &#39;如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。&#39;]
文本特征抽取的结果：
 [[ 0.          0.          0.          0.43643578  0.          0.          0.
   0.          0.          0.21821789  0.          0.21821789  0.          0.
   0.          0.          0.21821789  0.21821789  0.          0.43643578
   0.          0.21821789  0.          0.43643578  0.21821789  0.          0.
   0.          0.21821789  0.21821789  0.          0.          0.21821789
   0.        ]
 [ 0.2410822   0.          0.          0.          0.2410822   0.2410822
   0.2410822   0.          0.          0.          0.          0.          0.
   0.          0.2410822   0.55004769  0.          0.          0.          0.
   0.2410822   0.          0.          0.          0.          0.48216441
   0.          0.          0.          0.          0.          0.2410822
   0.          0.2410822 ]
 [ 0.          0.644003    0.48300225  0.          0.          0.          0.
   0.16100075  0.16100075  0.          0.16100075  0.          0.16100075
   0.16100075  0.          0.12244522  0.          0.          0.16100075
   0.          0.          0.          0.16100075  0.          0.          0.
   0.3220015   0.16100075  0.          0.          0.16100075  0.          0.
   0.        ]]
返回特征名字：
 [&#39;之前&#39;, &#39;了解&#39;, &#39;事物&#39;, &#39;今天&#39;, &#39;光是在&#39;, &#39;几百万年&#39;, &#39;发出&#39;, &#39;取决于&#39;, &#39;只用&#39;, &#39;后天&#39;, &#39;含义&#39;, &#39;大部分&#39;, &#39;如何&#39;, &#39;如果&#39;, &#39;宇宙&#39;, &#39;我们&#39;, &#39;所以&#39;, &#39;放弃&#39;, &#39;方式&#39;, &#39;明天&#39;, &#39;星系&#39;, &#39;晚上&#39;, &#39;某样&#39;, &#39;残酷&#39;, &#39;每个&#39;, &#39;看到&#39;, &#39;真正&#39;, &#39;秘密&#39;, &#39;绝对&#39;, &#39;美好&#39;, &#39;联系&#39;, &#39;过去&#39;, &#39;还是&#39;, &#39;这样&#39;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Tf-idf的重要性"><a href="#Tf-idf的重要性" class="headerlink" title="Tf-idf的重要性"></a>Tf-idf的重要性</h4><p><strong>分类机器学习算法进行文章分类中前期数据处理方式</strong></p>
<h3 id="无量纲化处理—标准化"><a href="#无量纲化处理—标准化" class="headerlink" title="无量纲化处理—标准化"></a>无量纲化处理—标准化</h3><p><strong>注意：无量纲化包括归一化和标准化，因为经常使用标准化所以这里只介绍标准化。</strong></p>
<h4 id="为什么我们要进行标准化？"><a href="#为什么我们要进行标准化？" class="headerlink" title="为什么我们要进行标准化？"></a>为什么我们要进行标准化？</h4><ul>
<li>特征的<strong>单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级</strong>，<strong>容易影响（支配）目标结果</strong>，使得一些算法无法学习到其它的特征</li>
</ul>
<p><img src="/posts/8/2.png" alt="约会对象数据"></p>
<p>我们需要用到一些方法进行<strong>无量纲化</strong>，<strong>使不同规格的数据转换到同一规格</strong></p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li>通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内</li>
</ul>
<h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><ul>
<li>sklearn.preprocessing.StandardScaler( )<ul>
<li>处理之后每列来说所有数据都聚集在均值0附近标准差差为1</li>
<li>StandardScaler.fit_transform(X)<ul>
<li>X:numpy array格式的数据[n_samples,n_features]</li>
</ul>
</li>
<li>返回值：转换后的形状相同的array</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import pandas as pd
from sklearn.preprocessing import StandardScaler    # 标准化

def stand_demo():
    &quot;&quot;&quot;
    标准化演示
    :return: None
    &quot;&quot;&quot;
    data &#x3D; pd.read_csv(&quot;dating.txt&quot;)
    print(data)
    # 1、实例化一个转换器类
    transfer &#x3D; StandardScaler()
    # 2、调用fit_transform
    data &#x3D; transfer.fit_transform(data[[&#39;milage&#39;,&#39;Liters&#39;,&#39;Consumtime&#39;]])
    print(&quot;标准化的结果:\n&quot;, data)
    print(&quot;每一列特征的平均值：\n&quot;, transfer.mean_)
    print(&quot;每一列特征的方差：\n&quot;, transfer.var_)

    return None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>返回结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">     milage     Liters  Consumtime  target
0     40920   8.326976    0.953952       3
1     14488   7.153469    1.673904       2
2     26052   1.441871    0.805124       1
..      ...        ...         ...     ...
997   26575  10.650102    0.866627       3
998   48111   9.134528    0.728045       3
999   43757   7.882601    1.332446       3

[1000 rows x 4 columns]
标准化的结果:
 [[ 0.33193158  0.41660188  0.24523407]
 [-0.87247784  0.13992897  1.69385734]
 [-0.34554872 -1.20667094 -0.05422437]
 ..., 
 [-0.32171752  0.96431572  0.06952649]
 [ 0.65959911  0.60699509 -0.20931587]
 [ 0.46120328  0.31183342  1.00680598]]
每一列特征的平均值：
 [  3.36354210e+04   6.55996083e+00   8.32072997e-01]
每一列特征的方差：
 [  4.81628039e+08   1.79902874e+01   2.46999554e-01]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="标准化总结"><a href="#标准化总结" class="headerlink" title="标准化总结"></a>标准化总结</h4><p>在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</p>
<h2 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h2><h4 id="什么是主成分分析-PCA"><a href="#什么是主成分分析-PCA" class="headerlink" title="什么是主成分分析(PCA)"></a>什么是主成分分析(PCA)</h4><ul>
<li>定义：<strong>高维数据转化为低维数据的过程</strong>，在此过程中<strong>可能会舍弃原有数据、创造新的变量</strong></li>
<li>作用：<strong>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</strong></li>
<li>应用：回归分析或者聚类分析当中</li>
</ul>
<blockquote>
<p>对于信息一词，在决策树中会进行介绍</p>
</blockquote>
<p>那么更好的理解这个过程呢？我们来看一张图</p>
<p><img src="/posts/8/3.png" alt="PCA解释图"></p>
<h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><ul>
<li>sklearn.decomposition.PCA(n_components=None)<ul>
<li>将数据分解为较低维数空间</li>
<li>n_components:<ul>
<li><strong>小数：表示保留百分之多少的信息</strong></li>
<li><strong>整数：减少到多少特征</strong></li>
</ul>
</li>
<li>PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]</li>
<li>返回值：转换后指定维度的array</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.decomposition import PCA    # PCA


def pca_demo():
    &quot;&quot;&quot;
    对数据进行PCA降维
    :return: None
    &quot;&quot;&quot;
    data &#x3D; [[2,8,4,5], [6,3,0,8], [5,4,9,1]]

    # 1、实例化PCA, 小数——保留多少信息
    transfer &#x3D; PCA(n_components&#x3D;0.9)
    # 2、调用fit_transform
    data1 &#x3D; transfer.fit_transform(data)

    print(&quot;保留90%的信息，降维结果为：\n&quot;, data1)

    # 1、实例化PCA, 整数——指定降维到的维数
    transfer2 &#x3D; PCA(n_components&#x3D;3)
    # 2、调用fit_transform
    data2 &#x3D; transfer2.fit_transform(data)
    print(&quot;降维到3维的结果：\n&quot;, data2)

    return None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>返回结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">保留90%的信息，降维结果为：
 [[ -3.13587302e-16   3.82970843e+00]
 [ -5.74456265e+00  -1.91485422e+00]
 [  5.74456265e+00  -1.91485422e+00]]
降维到3维的结果：
 [[ -3.13587302e-16   3.82970843e+00   4.59544715e-16]
 [ -5.74456265e+00  -1.91485422e+00   4.59544715e-16]
 [  5.74456265e+00  -1.91485422e+00   4.59544715e-16]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>




<h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><h3 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>机器学习一般的数据集会划分为两个部分：</p>
<ul>
<li>训练数据：用于训练，构建模型</li>
<li>测试数据：在模型检验时使用，用于评估模型是否有效</li>
</ul>
<p>划分比例：</p>
<ul>
<li>训练集：70% 80% 75%</li>
<li>测试集：30% 20% 30%</li>
</ul>
<h4 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h4><ul>
<li>sklearn.model_selection.train_test_split(arrays, *options)<ul>
<li>x 数据集的特征值</li>
<li>y 数据集的标签值</li>
<li>test_size 测试集的大小，一般为float</li>
<li>random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</li>
<li>return ，测试集特征训练集特征值值，训练标签，测试标签(默认随机取)</li>
</ul>
</li>
</ul>
<p>结合后面的数据集作介绍</p>
<h3 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h3><h4 id="K-近邻算法定义"><a href="#K-近邻算法定义" class="headerlink" title="K-近邻算法定义"></a>K-近邻算法定义</h4><ul>
<li>如果一个样本在特征空间中的<strong>k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别</strong>，则该样本也属于这个类别。</li>
</ul>
<h4 id="距离公式"><a href="#距离公式" class="headerlink" title="距离公式"></a>距离公式</h4><ul>
<li>两个样本的距离可以通过如下公式计算，又叫欧式距离</li>
</ul>
<p><img src="/posts/8/4.png" alt="距离公式"></p>
<h4 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h4><p>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</p>
<ul>
<li>n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数</li>
<li>algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)<pre class="line-numbers language-python" data-language="python"><code class="language-python">&quot;&quot;&quot;
1. 获取数据
2. 数据集划分
3. 特征工程---标准化
4. KNN预估器流程
5. 模型评估
&quot;&quot;&quot;

from sklearn.datasets import load_iris    # 数据集
from sklearn.model_selection import train_test_split    # 数据集划分
from sklearn.preprocessing import StandardScaler    # 标准化
from sklearn.neighbors import KNeighborsClassifier    # KNN算法


def knn_iris():
    &quot;&quot;&quot;
    用KNN算法对鸢尾花进行分类
    &quot;&quot;&quot;
    # 1. 获取数据
    iris &#x3D; load_iris()
    # 2. 数据集划分
    &quot;&quot;&quot;
    x是特征值 y是目标值
    训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test
    &quot;&quot;&quot;
    # random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
    x_train, x_test, y_train, y_test &#x3D; train_test_split(
        iris.data, iris.target, random_state&#x3D;6)
    # 3. 特征工程---标准化
    # 训练集和测试集都要进行标准化
    transfer &#x3D; StandardScaler()
    # 训练集标准化
    x_train &#x3D; transfer.fit_transform(x_train)
    # 测试集标准化    注意与训练集标准化的区分
    x_test &#x3D; transfer.transform(x_test)
    # 4. KNN预估器流程
    # n_neighbors就是k值
    estimator &#x3D; KNeighborsClassifier(n_neighbors&#x3D;3)
    estimator.fit(x_train, y_train)
    # 5. 模型评估
    # 方法1：直接比对真实值和预测值
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;y_predict:\n&quot;, y_predict)
    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)

    # 方法2：计算准确率
    score &#x3D; estimator.score(x_test, y_test)
    print(&quot;准确率为：\n&quot;, score)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    knn_iris()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
结果：</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict:
 [0 2 0 0 2 1 1 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
直接比对真实值和预测值:
 [ True  True  True  True  True  True False  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率为：
 0.9210526315789473<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h3><p>模型选择与调优这里有”交叉验证“和”超参数搜索（网格搜索）“两种，通过调用sklearn中的函数可以直接同时实现这两种功能。</p>
<h4 id="API-4"><a href="#API-4" class="headerlink" title="API"></a>API</h4><ul>
<li>sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)<ul>
<li>对估计器的指定参数值进行详尽搜索</li>
<li>estimator：估计器对象</li>
<li>param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}</li>
<li>cv：指定几折交叉验证</li>
<li></li>
<li>fit：输入训练数据</li>
<li>score：准确率</li>
<li>结果分析：<ul>
<li>best<em>score</em>:在交叉验证中验证的最好结果_</li>
<li>best<em>estimator</em>：最好的参数模型</li>
<li>cv<em>results</em>:每次交叉验证后的验证集准确率结果和训练集准确率结果</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&quot;&quot;&quot;
1. 获取数据
2. 数据集划分
3. 特征工程---标准化
4. KNN预估器流程
5. 模型评估
&quot;&quot;&quot;

from sklearn.datasets import load_iris    # 数据集
from sklearn.model_selection import train_test_split    # 数据集划分
from sklearn.preprocessing import StandardScaler    # 标准化
from sklearn.neighbors import KNeighborsClassifier    # KNN算法
from sklearn.model_selection import GridSearchCV    # 模型优化


def knn_iris_gscv():
    &quot;&quot;&quot;
    用KNN算法对鸢尾花进行分类
    使用交叉验证和网格搜索进行模型优化与调优
    &quot;&quot;&quot;
    iris &#x3D; load_iris()
    x_train, x_test, y_train, y_test &#x3D; train_test_split(
        iris.data, iris.target, random_state&#x3D;6)

    transfer &#x3D; StandardScaler()
    x_train &#x3D; transfer.fit_transform(x_train)
    x_test &#x3D; transfer.transform(x_test)

    estimator &#x3D; KNeighborsClassifier()

    # 加入网格搜索与交叉验证
    param_dict &#x3D; &#123;&quot;n_neighbors&quot;: [1, 3, 5, 7, 9, 11]&#125;
    # param_grid：估计器参数(dict) 例如：&#123;“n_neighbors”:[1,3,5]&#125;
    # cv：指定几折交叉验证
    estimator &#x3D; GridSearchCV(estimator, param_grid&#x3D;param_dict, cv&#x3D;10)

    estimator.fit(x_train, y_train)
    # 模型评估
    # 方法1：直接比对真实值和预测值
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;y_predict:\n&quot;, y_predict)
    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)

    # 方法2：计算准确率
    score &#x3D; estimator.score(x_test, y_test)
    print(&quot;准确率为：\n&quot;, score)

    # 最佳参数：best_params_
    print(&quot;最佳参数：\n&quot;, estimator.best_params_)
    # 最佳结果：best_score_
    print(&quot;最佳结果：\n&quot;, estimator.best_score_)
    # 最佳估计器：best_estimator_
    print(&quot;最佳估计器:\n&quot;, estimator.best_estimator_)
    # 交叉验证结果：cv_results_
    print(&quot;交叉验证结果:\n&quot;, estimator.cv_results_)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    knn_iris_gscv()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict:
 [0 2 0 0 2 1 2 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率为：
 0.9473684210526315
最佳参数：
 &#123;&#39;n_neighbors&#39;: 11&#125;
最佳结果：
 0.9734848484848484
最佳估计器:
 KNeighborsClassifier(n_neighbors&#x3D;11)
交叉验证结果:
 &#123;&#39;mean_fit_time&#39;: array([0.00069556, 0.00049865, 0.0005944 , 0.00069988, 0.00059845,
       0.0003933 ]), &#39;std_fit_time&#39;: array([0.00045544, 0.00049865, 0.00048581, 0.00045873, 0.00048918,
       0.00048186]), &#39;mean_score_time&#39;: array([0.00119884, 0.00119665, 0.00120168, 0.00109487, 0.0012959 ,
       0.00129161]), &#39;std_score_time&#39;: array([0.00039571, 0.00039928, 0.00038471, 0.00030016, 0.00044617,
       0.00045765]), &#39;param_n_neighbors&#39;: masked_array(data&#x3D;[1, 3, 5, 7, 9, 11],
             mask&#x3D;[False, False, False, False, False, False],
       fill_value&#x3D;&#39;?&#39;,
            dtype&#x3D;object), &#39;params&#39;: [&#123;&#39;n_neighbors&#39;: 1&#125;, &#123;&#39;n_neighbors&#39;: 3&#125;, &#123;&#39;n_neighbors&#39;: 5&#125;, &#123;&#39;n_neighbors&#39;: 7&#125;, &#123;&#39;n_neighbors&#39;: 9&#125;, &#123;&#39;n_neighbors&#39;: 11&#125;], &#39;split0_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split1_test_score&#39;: array([0.91666667, 0.91666667, 1.        , 0.91666667, 0.91666667,
       0.91666667]), &#39;split2_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split3_test_score&#39;: array([1.        , 1.        , 1.        , 1.        , 0.90909091,
       1.        ]), &#39;split4_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split5_test_score&#39;: array([0.90909091, 0.90909091, 1.        , 1.        , 1.        ,
       1.        ]), &#39;split6_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split7_test_score&#39;: array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 1.        ,
       1.        ]), &#39;split8_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split9_test_score&#39;: array([0.90909091, 0.81818182, 0.81818182, 0.81818182, 0.81818182,
       0.81818182]), &#39;mean_test_score&#39;: array([0.96439394, 0.95530303, 0.97272727, 0.96439394, 0.96439394,
       0.97348485]), &#39;std_test_score&#39;: array([0.04365767, 0.0604591 , 0.05821022, 0.05965639, 0.05965639,
       0.05742104]), &#39;rank_test_score&#39;: array([5, 6, 2, 3, 3, 1])&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>与不使用模型优化的KNN算法相对比，可以发现使用模型优化的KNN算法的结果更好。</p>
<h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><p>朴素贝叶斯算法多用于文本分类。</p>
<h4 id="API-5"><a href="#API-5" class="headerlink" title="API"></a>API</h4><ul>
<li>sklearn.naive_bayes.MultinomialNB(alpha = 1.0)<ul>
<li>朴素贝叶斯分类</li>
<li>alpha：拉普拉斯平滑系数</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&quot;&quot;&quot;
朴素贝叶斯算法
应用场景：文本分类  单词作为特征
优点：
    对缺失数据不太敏感，算法也比较简单，常用于文本分类。
    分类准确度高，速度快
缺点：
    由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好
&quot;&quot;&quot;
from sklearn.datasets import fetch_20newsgroups    # 获取数据集
from sklearn.model_selection import train_test_split    # 数据集划分
from sklearn.model_selection import GridSearchCV    # 模型优化
from sklearn.naive_bayes import MultinomialNB    # 朴素贝叶斯
from sklearn.feature_extraction.text import TfidfVectorizer    # 文本特征抽取


def nb_news():
    # 1. 获取数据
    news &#x3D; fetch_20newsgroups(subset&#x3D;&quot;all&quot;)
    # 2.划分数据集
    x_train, x_test, y_train, y_test &#x3D; train_test_split(news.data, news.target)
    transfer &#x3D; TfidfVectorizer()
    # 标准化
    x_train &#x3D; transfer.fit_transform(x_train)
    x_test &#x3D; transfer.transform(x_test)

    estimator &#x3D; MultinomialNB()

    # 模型优化
    param_dict &#x3D; &#123;&quot;alpha&quot;: [1, 3, 5, 7, 9, 11]&#125;
    estimator &#x3D; GridSearchCV(estimator, param_grid&#x3D;param_dict, cv&#x3D;10)

    estimator.fit(x_train, y_train)
    # 5. 模型评估
    # 方法1：直接比对真实值和预测值
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;y_predict:\n&quot;, y_predict)
    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)

    # 方法2：计算准确率
    score &#x3D; estimator.score(x_test, y_test)
    print(&quot;准确率为：\n&quot;, score)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    nb_news()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict:
 [ 9  4 17 ...  8  2  0]
直接比对真实值和预测值:
 [ True  True  True ...  True  True  True]
准确率为：
 0.8548387096774194<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="API-6"><a href="#API-6" class="headerlink" title="API"></a>API</h4><ul>
<li>class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)<ul>
<li>决策树分类器</li>
<li>criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’</li>
<li>max_depth:树的深度大小</li>
<li>random_state:随机数种子</li>
</ul>
</li>
<li>其中会有些超参数：max_depth:树的深度大小<ul>
<li>其它超参数我们会结合随机森林讲解</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_iris    # 数据集
from sklearn.model_selection import train_test_split    # 数据集划分
from sklearn.tree import DecisionTreeClassifier    # 决策树


def decision_iris():
    &quot;&quot;&quot;
    用决策树对鸢尾花进行分类
    优点：可视化 - 可解释能力强
    缺点：如果数据很多还不设置树的深度这样可能会产生过拟合。
    &quot;&quot;&quot;

    # 1. 获取数据集
    iris &#x3D; load_iris()

    # 2. 划分数据集
    x_train, x_test, y_train, y_test &#x3D; train_test_split(iris.data, iris.target)

    # 3. 决策树预估器
    # 参数max_depth:树的深度大小 如果数据集大则树的深度也大这样可能会导致准确率降低
    # 所以如果有必要的话可以设置树的深度来提高准确率
    estimator &#x3D; DecisionTreeClassifier(criterion&#x3D;&quot;entropy&quot;)
    estimator.fit(x_train, y_train)
    # 4. 模型评估
    # 方法1：直接比对真实值和预测值
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;y_predict:\n&quot;, y_predict)
    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)

    # 方法2：计算准确率
    score &#x3D; estimator.score(x_test, y_test)
    print(&quot;准确率为：\n&quot;, score)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    decision_iris()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict:
 [0 2 1 0 1 0 0 0 2 1 2 1 2 2 1 1 1 0 0 1 2 2 0 0 2 1 0 2 1 2 1 0 1 1 0 1 1
 1]
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True False  True  True  True False  True  True  True  True
  True  True  True  True False False  True  True  True  True  True False
  True  True]
准确率为：
 0.868421052631579<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><h4 id="随机森林原理过程"><a href="#随机森林原理过程" class="headerlink" title="随机森林原理过程"></a>随机森林原理过程</h4><p>学习算法根据下列算法而建造每棵树：</p>
<ul>
<li>用N来表示训练用例（样本）的个数，M表示特征数目。<ul>
<li>1、一次随机选出一个样本，重复N次， （有可能出现重复的样本）</li>
<li>2、随机去选出m个特征, m &lt;&lt;M，建立决策树</li>
</ul>
</li>
<li>采取bootstrap抽样</li>
</ul>
<h4 id="为什么采用BootStrap抽样"><a href="#为什么采用BootStrap抽样" class="headerlink" title="为什么采用BootStrap抽样"></a>为什么采用BootStrap抽样</h4><ul>
<li>为什么要随机抽样训练集？　　<ul>
<li>如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的</li>
</ul>
</li>
<li>为什么要有放回地抽样？<ul>
<li>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。</li>
</ul>
</li>
</ul>
<h4 id="API-7"><a href="#API-7" class="headerlink" title="API"></a>API</h4><ul>
<li><p>class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)</p>
<ul>
<li>随机森林分类器</li>
<li>n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</li>
<li>criteria：string，可选（default =“gini”）分割特征的测量方法</li>
<li>max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</li>
<li>max_features=”auto”,每个决策树的最大特征数量<ul>
<li>If “auto”, then <code>max_features=sqrt(n_features)</code>.</li>
<li>If “sqrt”, then <code>max_features=sqrt(n_features)</code> (same as “auto”).</li>
<li>If “log2”, then <code>max_features=log2(n_features)</code>.</li>
<li>If None, then <code>max_features=n_features</code>.</li>
</ul>
</li>
<li>bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样</li>
<li>min_samples_split:节点划分最少样本数</li>
<li>min_samples_leaf:叶子节点的最小样本数</li>
</ul>
</li>
<li><p>超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</p>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_iris    # 数据集
from sklearn.model_selection import train_test_split    # 数据集划分
from sklearn.ensemble import RandomForestClassifier    # 随机森林
from sklearn.model_selection import GridSearchCV    # 模型优化


def iris_demo():
    # 加载数据集
    iris &#x3D; load_iris()

    # 数据集划分
    x_train, x_test, y_train, y_test &#x3D; train_test_split(iris.data, iris.target)

    # 随机森林
    estimator &#x3D; RandomForestClassifier()

    # 网格化搜索
    param_dict &#x3D; &#123;&quot;n_estimators&quot;: [10, 20, 30, 40, 50, 100]&#125;
    estimator &#x3D; GridSearchCV(estimator, param_grid&#x3D;param_dict, cv&#x3D;10)

    # 训练
    estimator.fit(x_train, y_train)

    # 模型评估
    # 方法1：直接比对真实值和预测值
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;y_predict:\n&quot;, y_predict)
    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)

    # 方法2：计算准确率
    score &#x3D; estimator.score(x_test, y_test)
    print(&quot;准确率为：\n&quot;, score)
    # 最佳参数：best_params_
    print(&quot;最佳参数：\n&quot;, estimator.best_params_)
    # 最佳结果：best_score_
    print(&quot;最佳结果：\n&quot;, estimator.best_score_)
    # 最佳估计器：best_estimator_
    print(&quot;最佳估计器:\n&quot;, estimator.best_estimator_)
    # 交叉验证结果：cv_results_
    print(&quot;交叉验证结果:\n&quot;, estimator.cv_results_)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    iris_demo()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict:
 [0 1 2 2 1 2 2 1 2 2 0 0 0 0 0 0 1 2 0 2 1 2 2 0 0 0 1 2 2 2 1 1 1 1 0 2 1
 2]
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True  True  True  True  True  True  
  True  True  True  True  True  True  True  True  True  True  True  True   
  True  True  True  True  True  True False  True  True  True  True  True   
  True  True]
准确率为：
 0.9736842105263158
最佳参数：
 &#123;&#39;n_estimators&#39;: 30&#125;
最佳结果：
 0.9553030303030303
最佳估计器:
 RandomForestClassifier(n_estimators&#x3D;30)
交叉验证结果:
 &#123;&#39;mean_fit_time&#39;: array([0.01545861, 0.0299505 , 0.04111712, 0.05707047, 0.06866693,
       0.145067  ]), &#39;std_fit_time&#39;: array([0.00215007, 0.00540963, 0.00032881, 0.00359371, 0.00326092,
       0.0084516 ]), &#39;mean_score_time&#39;: array([0.00159588, 0.0027936 , 0.00379615, 0.00488796, 0.00544808,
       0.01146989]), &#39;std_score_time&#39;: array([0.00048815, 0.00040718, 0.00039285, 0.00054974, 0.00046851,
       0.00091972]), &#39;param_n_estimators&#39;: masked_array(data&#x3D;[10, 20, 30, 40, 50, 100],
             mask&#x3D;[False, False, False, False, False, False],
       fill_value&#x3D;&#39;?&#39;,
            dtype&#x3D;object), &#39;params&#39;: [&#123;&#39;n_estimators&#39;: 10&#125;, &#123;&#39;n_estimators&#39;: 20&#125;, &#123;&#39;n_estimators&#39;: 30&#125;, &#123;&#39;n_estimators&#39;: 40&#125;, &#123;&#39;n_estimators&#39;: 50&#125;, &#123;&#39;n_estimators&#39;: 100&#125;], &#39;split0_test_score&#39;: array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,
       0.91666667]), &#39;split1_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split2_test_score&#39;: array([0.90909091, 1.        , 1.        , 1.        , 1.        ,
       1.        ]), &#39;split3_test_score&#39;: array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,
       0.90909091]), &#39;split4_test_score&#39;: array([0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,
       0.81818182]), &#39;split5_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split6_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split7_test_score&#39;: array([0.90909091, 0.90909091, 1.        , 0.90909091, 1.        ,
       1.        ]), &#39;split8_test_score&#39;: array([1.        , 0.90909091, 1.        , 0.90909091, 1.        ,
       1.        ]), &#39;split9_test_score&#39;: array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,
       0.90909091]), &#39;mean_test_score&#39;: array([0.93712121, 0.93712121, 0.95530303, 0.93712121, 0.95530303,
       0.95530303]), &#39;std_test_score&#39;: array([0.05789881, 0.05789881, 0.0604591 , 0.05789881, 0.0604591 ,
       0.0604591 ]), &#39;rank_test_score&#39;: array([4, 4, 1, 4, 1, 1])&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>在当前所有算法中，具有极好的准确率</li>
<li>能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维</li>
<li>能够评估各个特征在分类问题上的重要性</li>
</ul>
<h2 id="回归与聚类算法"><a href="#回归与聚类算法" class="headerlink" title="回归与聚类算法"></a>回归与聚类算法</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h4><ul>
<li>sklearn.linear_model.LinearRegression(fit_intercept=True)<ul>
<li>通过正规方程优化</li>
<li>fit_intercept：是否计算偏置</li>
<li>LinearRegression.coef_：回归系数</li>
<li>LinearRegression.intercept_：偏置</li>
</ul>
</li>
<li>sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01)<ul>
<li>SGDRegressor类实现了随机梯度下降学习，它支持不同的<strong>loss函数和正则化惩罚项</strong>来拟合线性回归模型。</li>
<li>loss:损失类型<ul>
<li><strong>loss=”squared_loss”: 普通最小二乘法</strong></li>
</ul>
</li>
<li>fit_intercept：是否计算偏置</li>
<li>learning_rate : string, optional<ul>
<li>学习率填充</li>
<li><strong>‘constant’: eta = eta0</strong></li>
<li><strong>‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]</strong></li>
<li>‘invscaling’: eta = eta0 / pow(t, power_t)<ul>
<li><strong>power_t=0.25:存在父类当中</strong></li>
</ul>
</li>
<li><strong>对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</strong></li>
</ul>
</li>
<li>SGDRegressor.coef_：回归系数</li>
<li>SGDRegressor.intercept_：偏置</li>
</ul>
</li>
</ul>
<blockquote>
<p>sklearn提供给我们两种实现的API， 可以根据选择使用</p>
</blockquote>
<h4 id="回归性能评估"><a href="#回归性能评估" class="headerlink" title="回归性能评估"></a>回归性能评估</h4><p>sklearn.metrics.mean_squared_error(y_true, y_pred)</p>
<ul>
<li>均方误差回归损失</li>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return:浮点数结果</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_boston    # 加载数据集
from sklearn.model_selection import train_test_split    # 数据集划分
from sklearn.preprocessing import StandardScaler    # 特征工程 标准化
from sklearn.linear_model import LinearRegression, SGDRegressor    # 正规方程 梯度下降
from sklearn.metrics import mean_squared_error    # 回归评估 均方误差（值越小越好）


def linear1():
    &quot;&quot;&quot;
    正规方程的优化方法对波士顿房价进行预测
    &quot;&quot;&quot;
    boston &#x3D; load_boston()
    x_train, x_test, y_train, y_test &#x3D; train_test_split(
        boston.data, boston.target, random_state&#x3D;22)
    transfer &#x3D; StandardScaler()
    x_train &#x3D; transfer.fit_transform(x_train)
    x_test &#x3D; transfer.transform(x_test)
    estimator &#x3D; LinearRegression()
    estimator.fit(x_train, y_train)
    print(&quot;正规方程-权重系数为：\n&quot;, estimator.coef_)
    print(&quot;正规方程-偏置为：\n&quot;, estimator.intercept_)
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;正规方程-预测房价：\n&quot;, y_predict)
    error &#x3D; mean_squared_error(y_test, y_predict)
    print(&quot;正规方程-均方误差：\n&quot;, error)
    return None


def linear2():
    &quot;&quot;&quot;
    梯度下降的优化方法对波士顿房价进行预测
    &quot;&quot;&quot;
    boston &#x3D; load_boston()
    x_train, x_test, y_train, y_test &#x3D; train_test_split(
        boston.data, boston.target, random_state&#x3D;22)
    transfer &#x3D; StandardScaler()
    x_train &#x3D; transfer.fit_transform(x_train)
    x_test &#x3D; transfer.transform(x_test)
    estimator &#x3D; SGDRegressor()
    estimator.fit(x_train, y_train)
    print(&quot;梯度下降-权重系数为：\n&quot;, estimator.coef_)
    print(&quot;梯度下降-偏置为：\n&quot;, estimator.intercept_)
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;梯度下降-预测房价：\n&quot;, y_predict)
    error &#x3D; mean_squared_error(y_test, y_predict)
    print(&quot;梯度下降-均方误差：\n&quot;, error)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    linear1()
    linear2()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">正规方程-权重系数为：
 [-0.64817766  1.14673408 -0.05949444  0.74216553 -1.95515269  2.70902585
 -0.07737374 -3.29889391  2.50267196 -1.85679269 -1.75044624  0.87341624 
 -3.91336869]
正规方程-偏置为：
 22.62137203166228
正规方程-预测房价：
 [28.22944896 31.5122308  21.11612841 32.6663189  20.0023467  19.07315705
 21.09772798 19.61400153 19.61907059 32.87611987 20.97911561 27.52898011 
 15.54701758 19.78630176 36.88641203 18.81202132  9.35912225 18.49452615 
 30.66499315 24.30184448 19.08220837 34.11391208 29.81386585 17.51775647 
 34.91026707 26.54967053 34.71035391 27.4268996  19.09095832 14.92742976 
 30.86877936 15.88271775 37.17548808  7.72101675 16.24074861 17.19211608 
  7.42140081 20.0098852  40.58481466 28.93190595 25.25404307 17.74970308 
 38.76446932  6.87996052 21.80450956 25.29110265 20.427491   20.4698034  
 17.25330064 26.12442519  8.48268143 27.50871869 30.58284841 16.56039764 
  9.38919181 35.54434377 32.29801978 21.81298945 17.60263689 22.0804256  
 23.49262401 24.10617033 20.1346492  38.5268066  24.58319594 19.78072415 
 13.93429891  6.75507808 42.03759064 21.9215625  16.91352899 22.58327744 
 40.76440704 21.3998946  36.89912238 27.19273661 20.97945544 20.37925063 
 25.3536439  22.18729123 31.13342301 20.39451125 23.99224334 31.54729547 
 26.74581308 20.90199941 29.08225233 21.98331503 26.29101202 20.17329401 
 25.49225305 24.09171045 19.90739221 16.35154974 15.25184758 18.40766132 
 24.83797801 16.61703662 20.89470344 26.70854061 20.7591883  17.88403312 
 24.28656105 23.37651493 21.64202047 36.81476219 15.86570054 21.42338732 
 32.81366203 33.74086414 20.61688336 26.88191023 22.65739323 17.35731771 
 21.67699248 21.65034728 27.66728556 25.04691687 23.73976625 14.6649641  
 15.17700342  3.81620663 29.18194848 20.68544417 22.32934783 28.01568563 
 28.58237108]
正规方程-均方误差：
 20.6275137630954
梯度下降-权重系数为：
 [-0.52844184  0.94742828 -0.43896185  0.80011052 -1.70094588  2.83803954
 -0.15173308 -3.15314827  1.64459185 -0.92057374 -1.72092516  0.85822999
 -3.89079621]
梯度下降-偏置为：
 [22.62848948]
梯度下降-预测房价：
 [28.32292485 31.65691921 21.4754174  32.74532796 20.21502727 19.05803274
 21.38012862 19.40598106 19.65928004 32.83684983 21.37788237 27.27547404
 15.58434744 19.9543843  37.052139   18.66414622  9.64694632 18.60831548
 30.78911441 24.28055663 19.04506654 34.16527664 29.53666782 17.39899068
 34.86763757 26.51961514 34.42685759 27.38498522 19.1234001  15.69443705
 30.90416755 14.4629764  37.60886681  8.70471496 16.38818819 16.8388242
  7.70173349 19.7726613  40.59036302 29.18080616 25.25730687 17.82216674
 39.37499484  6.67884528 21.54796917 25.04507005 20.88378703 20.64882236
 17.03224546 26.32629603  9.6471961  27.18516688 30.67565735 16.69940767
  9.58964556 35.55768574 31.59626177 22.93550937 17.57965025 21.83043993
 23.62551177 23.94527301 20.3319314  38.24324246 25.72953353 19.68070738
 14.15726975  6.66109511 42.53435973 21.82504626 16.74834226 22.55184196
 41.01385643 21.71857114 36.98704919 27.16545173 21.83161702 20.78576019
 25.30987444 23.77003724 31.53599899 20.19287298 24.00693276 31.56432429
 27.2645538  20.86527222 29.1081147  21.95286144 26.74924411 18.76781794
 25.26458575 24.01985914 19.93042174 17.68510263 15.50813547 18.27522698
 24.59223379 16.73277678 20.66946955 26.79190915 20.72655782 17.97463708
 24.13232421 23.25410362 20.27914227 36.64368885 15.98551815 22.46864296
 32.71528599 33.73484378 20.54324486 25.9984172  23.32481418 17.74521574
 21.46651544 21.79381137 27.55126895 25.28602934 23.65813339 14.43211265
 15.64622064  3.64461024 29.25332792 20.65259117 22.31013756 28.06226938
 28.3763949 ]
梯度下降-均方误差：
 21.168264510208296<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>注意：均方误差越小表示效果越好。</p>
<h3 id="岭回归—线性回归的改进"><a href="#岭回归—线性回归的改进" class="headerlink" title="岭回归—线性回归的改进"></a>岭回归—线性回归的改进</h3><h4 id="API-8"><a href="#API-8" class="headerlink" title="API"></a>API</h4><p>sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=”auto”, normalize=False)</p>
<ul>
<li>具有l2正则化的线性回归</li>
<li>alpha:正则化力度，也叫 λ<ul>
<li><strong>λ取值：0<del>1 1</del>10</strong></li>
</ul>
</li>
<li>solver:会根据数据自动选择优化方法<ul>
<li><strong>sag:如果数据集、特征都比较大，选择该随机梯度下降优化</strong></li>
</ul>
</li>
<li>normalize:数据是否进行标准化<ul>
<li>normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</li>
</ul>
</li>
<li>Ridge.coef_:回归权重</li>
<li>Ridge.intercept_:回归偏置</li>
</ul>
<p><strong>Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</strong></p>
<ul>
<li>sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)<ul>
<li>具有l2正则化的线性回归，可以进行交叉验证</li>
<li>coef_:回归系数</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_boston  # 加载数据集
from sklearn.linear_model import Ridge  # 岭回归
from sklearn.metrics import mean_squared_error  # 回归评估 均方误差（值越小越好）
from sklearn.model_selection import train_test_split  # 数据集划分
from sklearn.preprocessing import StandardScaler  # 特征工程 标准化


def linear3():
    # 加载数据集
    boston &#x3D; load_boston()

    # 数据集划分
    x_train, x_test, y_train, y_test &#x3D; train_test_split(
        boston.data, boston.target, random_state&#x3D;22)

    # 数据集标准化
    transfer &#x3D; StandardScaler()
    x_train &#x3D; transfer.fit_transform(x_train)
    x_test &#x3D; transfer.transform(x_test)

    # 预估器
    estimator &#x3D; Ridge()
    estimator.fit(x_train, y_train)

    # 得出模型
    print(&quot;岭回归-权重系数为：\n&quot;, estimator.coef_)
    print(&quot;岭回归-偏置为：\n&quot;, estimator.intercept_)

    # 模型评估
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;岭回归-预测房价：\n&quot;, y_predict)
    error &#x3D; mean_squared_error(y_test, y_predict)
    print(&quot;岭回归-均方误差：\n&quot;, error)
    return None


if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    linear3()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">岭回归-权重系数为：
 [-0.63591916  1.12109181 -0.09319611  0.74628129 -1.91888749  2.71927719
 -0.08590464 -3.25882705  2.41315949 -1.76930347 -1.74279405  0.87205004 
 -3.89758657]
岭回归-偏置为：
 22.62137203166228
岭回归-预测房价：
 [28.22119941 31.49858594 21.14690941 32.64962343 20.03976087 19.07187629
 21.11827061 19.61935024 19.64669848 32.83666525 21.01034708 27.47939935
 15.55875601 19.80406014 36.86415472 18.79442579  9.42343608 18.5205955
 30.67129766 24.30659711 19.07820077 34.08772738 29.77396117 17.50394928
 34.87750492 26.52508961 34.65566473 27.42939944 19.08639183 15.04854291
 30.84974343 15.76894723 37.18814441  7.81864035 16.27847433 17.15510852
  7.46590141 19.98474662 40.55565604 28.96103939 25.25570196 17.7598197
 38.78171653  6.87935126 21.76805062 25.25888823 20.47319256 20.48808719
 17.24949519 26.11755181  8.61005188 27.47070495 30.57806886 16.57080888
  9.42312214 35.50731907 32.20467352 21.93128073 17.62011278 22.08454636
 23.50121152 24.08248876 20.16840581 38.47001591 24.69276673 19.7638548
 13.96547058  6.76070715 42.04033544 21.9237625  16.88030656 22.60637682
 40.74664535 21.44631815 36.86936185 27.17135794 21.09470367 20.40689317
 25.35934079 22.35676321 31.1513028  20.39303322 23.99948991 31.54251155
 26.77734347 20.89368871 29.05880401 22.00850263 26.31965286 20.04852734
 25.46476799 24.08084537 19.90846889 16.47030743 15.27936372 18.39475348
 24.80822272 16.62280764 20.86393724 26.70418608 20.74534996 17.89544942
 24.25949423 23.35743497 21.51817773 36.76202304 15.90293344 21.52915882
 32.78684766 33.68666117 20.61700911 26.78345059 22.72685584 17.40478038
 21.67136433 21.6912557  27.66684993 25.08825085 23.72539867 14.64260535
 15.21105331  3.81916568 29.16662813 20.67913144 22.33386579 28.01241753
 28.531445  ]
岭回归-均方误差：
 20.65644821435496<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="逻辑回归与二分类—分类算法"><a href="#逻辑回归与二分类—分类算法" class="headerlink" title="逻辑回归与二分类—分类算法"></a>逻辑回归与二分类—分类算法</h3><h4 id="逻辑回归API"><a href="#逻辑回归API" class="headerlink" title="逻辑回归API"></a>逻辑回归API</h4><ul>
<li>sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)<ul>
<li>solver:优化求解方式（默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数）<ul>
<li>sag：根据数据集自动选择，随机平均梯度下降</li>
</ul>
</li>
<li>penalty：正则化的种类</li>
<li>C：正则化力度</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>默认将类别数量少的当做正例</strong></p>
</blockquote>
<p><strong>LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)</strong></p>
<h4 id="分类评估报告API"><a href="#分类评估报告API" class="headerlink" title="分类评估报告API"></a>分类评估报告API</h4><ul>
<li>sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )</li>
<li><ul>
<li>y_true：真实目标值</li>
<li>y_pred：估计器预测目标值</li>
<li>labels:指定类别对应的数字</li>
<li>target_names：目标类别名称</li>
<li>return：每个类别精确率与召回率</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression  # 逻辑回归
from sklearn.metrics import classification_report, roc_auc_score    # 分类评估
from sklearn.model_selection import train_test_split  # 数据集划分
from sklearn.preprocessing import StandardScaler  # 特征工程 标准化

if __name__ &#x3D;&#x3D; &quot;__main__&quot;:
    # 加载数据
    column_name &#x3D; [&#39;Sample code number&#39;, &#39;Clump Thickness&#39;,
                   &#39;Uniformity of Cell Size&#39;, &#39;Uniformity of Cell Shape&#39;,
                   &#39;Marginal Adhesion&#39;, &#39;Single Epithelial Cell Size&#39;,
                   &#39;Bare Nuclei&#39;, &#39;Bland Chromatin&#39;,
                   &#39;Normal Nucleoli&#39;, &#39;Mitoses&#39;, &#39;Class&#39;]
    path &#x3D; r&quot;D:\痛苦の资料\不会真的有人学编程吧\机器学习\黑马程序员\自己的整理\day3\breast-cancer-wisconsin.data&quot;
    data &#x3D; pd.read_csv(path, names&#x3D;column_name)

    # 数据处理  因为数据中有缺失值，缺失的地方使用 ? 替代，目标是删掉缺失值
    data &#x3D; data.replace(to_replace&#x3D;&#39;?&#39;, value&#x3D;np.nan)
    data.dropna(inplace&#x3D;True)

    # 数据处理  获取特征值和目标值
    x &#x3D; data.iloc[:, 1:-1]
    y &#x3D; data[&quot;Class&quot;]

    # 数据集划分
    x_train, x_test, y_train, y_test &#x3D; train_test_split(x, y)

    # 数据标准化
    transfer &#x3D; StandardScaler()
    x_train &#x3D; transfer.fit_transform(x_train)
    x_test &#x3D; transfer.transform(x_test)

    # 预估器
    estimator &#x3D; LogisticRegression()
    estimator.fit(x_train, y_train)

    # 模型
    print(&quot;逻辑回归-权重系数为：\n&quot;, estimator.coef_)
    print(&quot;逻辑回归-偏置为：\n&quot;, estimator.intercept_)
    y_predict &#x3D; estimator.predict(x_test)
    print(&quot;y_predict：\n&quot;, y_predict)
    print(&quot;直接比对真实值和预测值：\n&quot;, y_test &#x3D;&#x3D; y_predict)
    score &#x3D; estimator.score(x_test, y_test)
    print(&quot;准确率：\n&quot;, score)

    # 查看精确率、召回率、F1-score
    report &#x3D; classification_report(y_test, y_predict, labels&#x3D;[
                                   2, 4], target_names&#x3D;[&#39;良性&#39;, &#39;恶性&#39;])

    # y_true:每个样本的真实类别，必须为0(反例),1(正例)标记
    # 将y_test 转换成 0 1
    y_true &#x3D; np.where(y_test &gt; 3, 1, 0)
    print(roc_auc_score(y_true, y_predict))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">逻辑回归-权重系数为：
 [[1.51911637 0.06401522 0.69614721 0.76445915 0.50264081 1.37783828
  1.13432057 0.6161716  0.85294448]]
逻辑回归-偏置为：
 [-0.90973535]
y_predict：
 [2 4 2 4 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 4
 2 4 2 4 4 2 2 4 4 4 4 4 2 2 2 4 2 4 4 2 2 2 2 4 2 2 2 4 2 4 4 2 2 2 2 2 2
 2 2 4 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 4 4 4 2 2 2 2 4 4 2 4 4 2 4
 2 4 4 4 2 2 2 2 2 2 4 2 4 2 2 2 2 2 4 2 4 4 2 4 2 2 4 4 4 2 2 2 2 2 2 2 2
 2 2 2 2 4 4 2 2 2 4 2 4 2 2 2 2 2 2 2 2 4 4 2]
直接比对真实值和预测值：
 137    True
506    True
578    True
100    True
600    True
       ...
426    True
390    True
336    True
289    True
444    True
Name: Class, Length: 171, dtype: bool
准确率：
 0.9649122807017544
0.9585488041370394<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="无监督学习—K-means算法"><a href="#无监督学习—K-means算法" class="headerlink" title="无监督学习—K-means算法"></a>无监督学习—K-means算法</h3><h4 id="K-meansAPI"><a href="#K-meansAPI" class="headerlink" title="K-meansAPI"></a>K-meansAPI</h4><ul>
<li>sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)<ul>
<li>k-means聚类</li>
<li>n_clusters:开始的聚类中心数量</li>
<li>init:初始化方法，默认为’k-means ++’</li>
<li>labels_:默认标记的类型，可以和真实值比较（不是值比较）</li>
</ul>
</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">LightningMaster</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://lightningleader.github.io/posts/8.html">https://lightningleader.github.io/posts/8.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">LightningMaster</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">学习笔记</span>
                                </a>
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">机器学习</span>
                                </a>
                            
                                <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                                    <span class="chip bg-color">人工智能</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'FH3ePeHtSYQs3mISGfKQWelY-gzGzoHsz',
        appKey: 'y77ukh1PWni9QKj8zfbCn84L',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/9.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/8.jpg" class="responsive-img" alt="整理收藏好用的宝藏网站">
                        
                        <span class="card-title">整理收藏好用的宝藏网站</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            整理收藏好用的宝藏网站
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-03-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            LightningMaster
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB/">
                        <span class="chip bg-color">资源分享</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/7.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/9.jpg" class="responsive-img" alt="OpenCV颜色识别 物体追踪">
                        
                        <span class="card-title">OpenCV颜色识别 物体追踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            OpenCV颜色识别 物体追踪
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            LightningMaster
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%95%99%E7%A8%8B/">
                        <span class="chip bg-color">教程</span>
                    </a>
                    
                    <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">学习笔记</span>
                    </a>
                    
                    <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">
                        <span class="chip bg-color">图像处理</span>
                    </a>
                    
                    <a href="/tags/OpenCV/">
                        <span class="chip bg-color">OpenCV</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2021</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">LightningMaster</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">25.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/LightningLeader" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:lightningleader@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!--动态线条背景-->
    <script type="text/javascript"
    color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>

</body>

</html>
