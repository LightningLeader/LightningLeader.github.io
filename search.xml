<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习入门笔记</title>
      <link href="posts/8.html"/>
      <url>posts/8.html</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习入门笔记"><a href="#机器学习入门笔记" class="headerlink" title="机器学习入门笔记"></a>机器学习入门笔记</h1><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个是根据“黑马程序员”的一套机器学习课程编写的博客。这篇博客上的案例均为课程里面的，但并没有包含所有案例，只是记录了部分个人认为比较重要的部分。</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="Tf-idf文本特征提取"><a href="#Tf-idf文本特征提取" class="headerlink" title="Tf-idf文本特征提取"></a>Tf-idf文本特征提取</h3><h4 id="Tf-idf介绍"><a href="#Tf-idf介绍" class="headerlink" title="Tf-idf介绍"></a>Tf-idf介绍</h4><ul><li><p>TF-IDF的主要思想是：如果<strong>某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现</strong>，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p></li><li><p><strong>TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</strong></p></li><li><p>词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率</p></li><li><p>逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以<strong>由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到</strong></p></li></ul><p><img src="/posts/8/1.png" alt="tfidf公式"></p><p>最终得出结果可以理解为重要程度。</p><p>注：假如一篇文件的总词语数是100个，而词语”非常”出现了5次，那么”非常”一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现”非常”一词的文件数。所以，如果”非常”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后”非常”对于这篇文档的tf-idf的分数为0.05 * 3=0.15</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer    # Tf-idf文本特征提取import jieba    # 分词模块def cut_word(text):    &quot;&quot;&quot;    对中文进行分词    &quot;我爱北京天安门&quot;————&gt;&quot;我 爱 北京 天安门&quot;    :param text:    :return: text    &quot;&quot;&quot;    # 用结巴对中文字符串进行分词    text &#x3D; &quot; &quot;.join(list(jieba.cut(text)))    return textdef text_chinese_tfidf_demo():    &quot;&quot;&quot;    对中文进行特征抽取    :return: None    &quot;&quot;&quot;    data &#x3D; [&quot;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。&quot;,            &quot;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。&quot;,            &quot;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。&quot;]    # 将原始数据转换成分好词的形式    text_list &#x3D; []    for sent in data:        text_list.append(cut_word(sent))    print(text_list)    # 1、实例化一个转换器类    # transfer &#x3D; CountVectorizer(sparse&#x3D;False)    transfer &#x3D; TfidfVectorizer(stop_words&#x3D;[&#39;一种&#39;, &#39;不会&#39;, &#39;不要&#39;])    # 2、调用fit_transform    data &#x3D; transfer.fit_transform(text_list)    print(&quot;文本特征抽取的结果：\n&quot;, data.toarray())    print(&quot;返回特征名字：\n&quot;, transfer.get_feature_names())    return None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>返回结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Building prefix dict from the default dictionary ...Loading model from cache &#x2F;var&#x2F;folders&#x2F;mz&#x2F;tzf2l3sx4rgg6qpglfb035_r0000gn&#x2F;T&#x2F;jieba.cacheLoading model cost 0.856 seconds.Prefix dict has been built succesfully.[&#39;一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。&#39;, &#39;我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。&#39;, &#39;如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。&#39;]文本特征抽取的结果： [[ 0.          0.          0.          0.43643578  0.          0.          0.   0.          0.          0.21821789  0.          0.21821789  0.          0.   0.          0.          0.21821789  0.21821789  0.          0.43643578   0.          0.21821789  0.          0.43643578  0.21821789  0.          0.   0.          0.21821789  0.21821789  0.          0.          0.21821789   0.        ] [ 0.2410822   0.          0.          0.          0.2410822   0.2410822   0.2410822   0.          0.          0.          0.          0.          0.   0.          0.2410822   0.55004769  0.          0.          0.          0.   0.2410822   0.          0.          0.          0.          0.48216441   0.          0.          0.          0.          0.          0.2410822   0.          0.2410822 ] [ 0.          0.644003    0.48300225  0.          0.          0.          0.   0.16100075  0.16100075  0.          0.16100075  0.          0.16100075   0.16100075  0.          0.12244522  0.          0.          0.16100075   0.          0.          0.          0.16100075  0.          0.          0.   0.3220015   0.16100075  0.          0.          0.16100075  0.          0.   0.        ]]返回特征名字： [&#39;之前&#39;, &#39;了解&#39;, &#39;事物&#39;, &#39;今天&#39;, &#39;光是在&#39;, &#39;几百万年&#39;, &#39;发出&#39;, &#39;取决于&#39;, &#39;只用&#39;, &#39;后天&#39;, &#39;含义&#39;, &#39;大部分&#39;, &#39;如何&#39;, &#39;如果&#39;, &#39;宇宙&#39;, &#39;我们&#39;, &#39;所以&#39;, &#39;放弃&#39;, &#39;方式&#39;, &#39;明天&#39;, &#39;星系&#39;, &#39;晚上&#39;, &#39;某样&#39;, &#39;残酷&#39;, &#39;每个&#39;, &#39;看到&#39;, &#39;真正&#39;, &#39;秘密&#39;, &#39;绝对&#39;, &#39;美好&#39;, &#39;联系&#39;, &#39;过去&#39;, &#39;还是&#39;, &#39;这样&#39;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Tf-idf的重要性"><a href="#Tf-idf的重要性" class="headerlink" title="Tf-idf的重要性"></a>Tf-idf的重要性</h4><p><strong>分类机器学习算法进行文章分类中前期数据处理方式</strong></p><h3 id="无量纲化处理—标准化"><a href="#无量纲化处理—标准化" class="headerlink" title="无量纲化处理—标准化"></a>无量纲化处理—标准化</h3><p><strong>注意：无量纲化包括归一化和标准化，因为经常使用标准化所以这里只介绍标准化。</strong></p><h4 id="为什么我们要进行标准化？"><a href="#为什么我们要进行标准化？" class="headerlink" title="为什么我们要进行标准化？"></a>为什么我们要进行标准化？</h4><ul><li>特征的<strong>单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级</strong>，<strong>容易影响（支配）目标结果</strong>，使得一些算法无法学习到其它的特征</li></ul><p><img src="/posts/8/2.png" alt="约会对象数据"></p><p>我们需要用到一些方法进行<strong>无量纲化</strong>，<strong>使不同规格的数据转换到同一规格</strong></p><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul><li>通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内</li></ul><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><ul><li>sklearn.preprocessing.StandardScaler( )<ul><li>处理之后每列来说所有数据都聚集在均值0附近标准差差为1</li><li>StandardScaler.fit_transform(X)<ul><li>X:numpy array格式的数据[n_samples,n_features]</li></ul></li><li>返回值：转换后的形状相同的array</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">import pandas as pdfrom sklearn.preprocessing import StandardScaler    # 标准化def stand_demo():    &quot;&quot;&quot;    标准化演示    :return: None    &quot;&quot;&quot;    data &#x3D; pd.read_csv(&quot;dating.txt&quot;)    print(data)    # 1、实例化一个转换器类    transfer &#x3D; StandardScaler()    # 2、调用fit_transform    data &#x3D; transfer.fit_transform(data[[&#39;milage&#39;,&#39;Liters&#39;,&#39;Consumtime&#39;]])    print(&quot;标准化的结果:\n&quot;, data)    print(&quot;每一列特征的平均值：\n&quot;, transfer.mean_)    print(&quot;每一列特征的方差：\n&quot;, transfer.var_)    return None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>返回结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">     milage     Liters  Consumtime  target0     40920   8.326976    0.953952       31     14488   7.153469    1.673904       22     26052   1.441871    0.805124       1..      ...        ...         ...     ...997   26575  10.650102    0.866627       3998   48111   9.134528    0.728045       3999   43757   7.882601    1.332446       3[1000 rows x 4 columns]标准化的结果: [[ 0.33193158  0.41660188  0.24523407] [-0.87247784  0.13992897  1.69385734] [-0.34554872 -1.20667094 -0.05422437] ...,  [-0.32171752  0.96431572  0.06952649] [ 0.65959911  0.60699509 -0.20931587] [ 0.46120328  0.31183342  1.00680598]]每一列特征的平均值： [  3.36354210e+04   6.55996083e+00   8.32072997e-01]每一列特征的方差： [  4.81628039e+08   1.79902874e+01   2.46999554e-01]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="标准化总结"><a href="#标准化总结" class="headerlink" title="标准化总结"></a>标准化总结</h4><p>在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</p><h2 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h2><h4 id="什么是主成分分析-PCA"><a href="#什么是主成分分析-PCA" class="headerlink" title="什么是主成分分析(PCA)"></a>什么是主成分分析(PCA)</h4><ul><li>定义：<strong>高维数据转化为低维数据的过程</strong>，在此过程中<strong>可能会舍弃原有数据、创造新的变量</strong></li><li>作用：<strong>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</strong></li><li>应用：回归分析或者聚类分析当中</li></ul><blockquote><p>对于信息一词，在决策树中会进行介绍</p></blockquote><p>那么更好的理解这个过程呢？我们来看一张图</p><p><img src="/posts/8/3.png" alt="PCA解释图"></p><h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><ul><li>sklearn.decomposition.PCA(n_components=None)<ul><li>将数据分解为较低维数空间</li><li>n_components:<ul><li><strong>小数：表示保留百分之多少的信息</strong></li><li><strong>整数：减少到多少特征</strong></li></ul></li><li>PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]</li><li>返回值：转换后指定维度的array</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.decomposition import PCA    # PCAdef pca_demo():    &quot;&quot;&quot;    对数据进行PCA降维    :return: None    &quot;&quot;&quot;    data &#x3D; [[2,8,4,5], [6,3,0,8], [5,4,9,1]]    # 1、实例化PCA, 小数——保留多少信息    transfer &#x3D; PCA(n_components&#x3D;0.9)    # 2、调用fit_transform    data1 &#x3D; transfer.fit_transform(data)    print(&quot;保留90%的信息，降维结果为：\n&quot;, data1)    # 1、实例化PCA, 整数——指定降维到的维数    transfer2 &#x3D; PCA(n_components&#x3D;3)    # 2、调用fit_transform    data2 &#x3D; transfer2.fit_transform(data)    print(&quot;降维到3维的结果：\n&quot;, data2)    return None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>返回结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">保留90%的信息，降维结果为： [[ -3.13587302e-16   3.82970843e+00] [ -5.74456265e+00  -1.91485422e+00] [  5.74456265e+00  -1.91485422e+00]]降维到3维的结果： [[ -3.13587302e-16   3.82970843e+00   4.59544715e-16] [ -5.74456265e+00  -1.91485422e+00   4.59544715e-16] [  5.74456265e+00  -1.91485422e+00   4.59544715e-16]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><h3 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>机器学习一般的数据集会划分为两个部分：</p><ul><li>训练数据：用于训练，构建模型</li><li>测试数据：在模型检验时使用，用于评估模型是否有效</li></ul><p>划分比例：</p><ul><li>训练集：70% 80% 75%</li><li>测试集：30% 20% 30%</li></ul><h4 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h4><ul><li>sklearn.model_selection.train_test_split(arrays, *options)<ul><li>x 数据集的特征值</li><li>y 数据集的标签值</li><li>test_size 测试集的大小，一般为float</li><li>random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</li><li>return ，测试集特征训练集特征值值，训练标签，测试标签(默认随机取)</li></ul></li></ul><p>结合后面的数据集作介绍</p><h3 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h3><h4 id="K-近邻算法定义"><a href="#K-近邻算法定义" class="headerlink" title="K-近邻算法定义"></a>K-近邻算法定义</h4><ul><li>如果一个样本在特征空间中的<strong>k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别</strong>，则该样本也属于这个类别。</li></ul><h4 id="距离公式"><a href="#距离公式" class="headerlink" title="距离公式"></a>距离公式</h4><ul><li>两个样本的距离可以通过如下公式计算，又叫欧式距离</li></ul><p><img src="/posts/8/4.png" alt="距离公式"></p><h4 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h4><p>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</p><ul><li>n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数</li><li>algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)<pre class="line-numbers language-python" data-language="python"><code class="language-python">&quot;&quot;&quot;1. 获取数据2. 数据集划分3. 特征工程---标准化4. KNN预估器流程5. 模型评估&quot;&quot;&quot;from sklearn.datasets import load_iris    # 数据集from sklearn.model_selection import train_test_split    # 数据集划分from sklearn.preprocessing import StandardScaler    # 标准化from sklearn.neighbors import KNeighborsClassifier    # KNN算法def knn_iris():    &quot;&quot;&quot;    用KNN算法对鸢尾花进行分类    &quot;&quot;&quot;    # 1. 获取数据    iris &#x3D; load_iris()    # 2. 数据集划分    &quot;&quot;&quot;    x是特征值 y是目标值    训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test    &quot;&quot;&quot;    # random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。    x_train, x_test, y_train, y_test &#x3D; train_test_split(        iris.data, iris.target, random_state&#x3D;6)    # 3. 特征工程---标准化    # 训练集和测试集都要进行标准化    transfer &#x3D; StandardScaler()    # 训练集标准化    x_train &#x3D; transfer.fit_transform(x_train)    # 测试集标准化    注意与训练集标准化的区分    x_test &#x3D; transfer.transform(x_test)    # 4. KNN预估器流程    # n_neighbors就是k值    estimator &#x3D; KNeighborsClassifier(n_neighbors&#x3D;3)    estimator.fit(x_train, y_train)    # 5. 模型评估    # 方法1：直接比对真实值和预测值    y_predict &#x3D; estimator.predict(x_test)    print(&quot;y_predict:\n&quot;, y_predict)    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)    # 方法2：计算准确率    score &#x3D; estimator.score(x_test, y_test)    print(&quot;准确率为：\n&quot;, score)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    knn_iris()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>结果：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict: [0 2 0 0 2 1 1 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1 2]直接比对真实值和预测值: [ True  True  True  True  True  True False  True  True  True  True  True  True  True  True False  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True False  True  True  True]准确率为： 0.9210526315789473<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h3><p>模型选择与调优这里有”交叉验证“和”超参数搜索（网格搜索）“两种，通过调用sklearn中的函数可以直接同时实现这两种功能。</p><h4 id="API-4"><a href="#API-4" class="headerlink" title="API"></a>API</h4><ul><li>sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)<ul><li>对估计器的指定参数值进行详尽搜索</li><li>estimator：估计器对象</li><li>param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}</li><li>cv：指定几折交叉验证</li><li></li><li>fit：输入训练数据</li><li>score：准确率</li><li>结果分析：<ul><li>best<em>score</em>:在交叉验证中验证的最好结果_</li><li>best<em>estimator</em>：最好的参数模型</li><li>cv<em>results</em>:每次交叉验证后的验证集准确率结果和训练集准确率结果</li></ul></li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">&quot;&quot;&quot;1. 获取数据2. 数据集划分3. 特征工程---标准化4. KNN预估器流程5. 模型评估&quot;&quot;&quot;from sklearn.datasets import load_iris    # 数据集from sklearn.model_selection import train_test_split    # 数据集划分from sklearn.preprocessing import StandardScaler    # 标准化from sklearn.neighbors import KNeighborsClassifier    # KNN算法from sklearn.model_selection import GridSearchCV    # 模型优化def knn_iris_gscv():    &quot;&quot;&quot;    用KNN算法对鸢尾花进行分类    使用交叉验证和网格搜索进行模型优化与调优    &quot;&quot;&quot;    iris &#x3D; load_iris()    x_train, x_test, y_train, y_test &#x3D; train_test_split(        iris.data, iris.target, random_state&#x3D;6)    transfer &#x3D; StandardScaler()    x_train &#x3D; transfer.fit_transform(x_train)    x_test &#x3D; transfer.transform(x_test)    estimator &#x3D; KNeighborsClassifier()    # 加入网格搜索与交叉验证    param_dict &#x3D; &#123;&quot;n_neighbors&quot;: [1, 3, 5, 7, 9, 11]&#125;    # param_grid：估计器参数(dict) 例如：&#123;“n_neighbors”:[1,3,5]&#125;    # cv：指定几折交叉验证    estimator &#x3D; GridSearchCV(estimator, param_grid&#x3D;param_dict, cv&#x3D;10)    estimator.fit(x_train, y_train)    # 模型评估    # 方法1：直接比对真实值和预测值    y_predict &#x3D; estimator.predict(x_test)    print(&quot;y_predict:\n&quot;, y_predict)    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)    # 方法2：计算准确率    score &#x3D; estimator.score(x_test, y_test)    print(&quot;准确率为：\n&quot;, score)    # 最佳参数：best_params_    print(&quot;最佳参数：\n&quot;, estimator.best_params_)    # 最佳结果：best_score_    print(&quot;最佳结果：\n&quot;, estimator.best_score_)    # 最佳估计器：best_estimator_    print(&quot;最佳估计器:\n&quot;, estimator.best_estimator_)    # 交叉验证结果：cv_results_    print(&quot;交叉验证结果:\n&quot;, estimator.cv_results_)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    knn_iris_gscv()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict: [0 2 0 0 2 1 2 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1 2]直接比对真实值和预测值: [ True  True  True  True  True  True  True  True  True  True  True  True  True  True  True False  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True  True False  True  True  True]准确率为： 0.9473684210526315最佳参数： &#123;&#39;n_neighbors&#39;: 11&#125;最佳结果： 0.9734848484848484最佳估计器: KNeighborsClassifier(n_neighbors&#x3D;11)交叉验证结果: &#123;&#39;mean_fit_time&#39;: array([0.00069556, 0.00049865, 0.0005944 , 0.00069988, 0.00059845,       0.0003933 ]), &#39;std_fit_time&#39;: array([0.00045544, 0.00049865, 0.00048581, 0.00045873, 0.00048918,       0.00048186]), &#39;mean_score_time&#39;: array([0.00119884, 0.00119665, 0.00120168, 0.00109487, 0.0012959 ,       0.00129161]), &#39;std_score_time&#39;: array([0.00039571, 0.00039928, 0.00038471, 0.00030016, 0.00044617,       0.00045765]), &#39;param_n_neighbors&#39;: masked_array(data&#x3D;[1, 3, 5, 7, 9, 11],             mask&#x3D;[False, False, False, False, False, False],       fill_value&#x3D;&#39;?&#39;,            dtype&#x3D;object), &#39;params&#39;: [&#123;&#39;n_neighbors&#39;: 1&#125;, &#123;&#39;n_neighbors&#39;: 3&#125;, &#123;&#39;n_neighbors&#39;: 5&#125;, &#123;&#39;n_neighbors&#39;: 7&#125;, &#123;&#39;n_neighbors&#39;: 9&#125;, &#123;&#39;n_neighbors&#39;: 11&#125;], &#39;split0_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split1_test_score&#39;: array([0.91666667, 0.91666667, 1.        , 0.91666667, 0.91666667,       0.91666667]), &#39;split2_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split3_test_score&#39;: array([1.        , 1.        , 1.        , 1.        , 0.90909091,       1.        ]), &#39;split4_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split5_test_score&#39;: array([0.90909091, 0.90909091, 1.        , 1.        , 1.        ,       1.        ]), &#39;split6_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split7_test_score&#39;: array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 1.        ,       1.        ]), &#39;split8_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split9_test_score&#39;: array([0.90909091, 0.81818182, 0.81818182, 0.81818182, 0.81818182,       0.81818182]), &#39;mean_test_score&#39;: array([0.96439394, 0.95530303, 0.97272727, 0.96439394, 0.96439394,       0.97348485]), &#39;std_test_score&#39;: array([0.04365767, 0.0604591 , 0.05821022, 0.05965639, 0.05965639,       0.05742104]), &#39;rank_test_score&#39;: array([5, 6, 2, 3, 3, 1])&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>与不使用模型优化的KNN算法相对比，可以发现使用模型优化的KNN算法的结果更好。</p><h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><p>朴素贝叶斯算法多用于文本分类。</p><h4 id="API-5"><a href="#API-5" class="headerlink" title="API"></a>API</h4><ul><li>sklearn.naive_bayes.MultinomialNB(alpha = 1.0)<ul><li>朴素贝叶斯分类</li><li>alpha：拉普拉斯平滑系数</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">&quot;&quot;&quot;朴素贝叶斯算法应用场景：文本分类  单词作为特征优点：    对缺失数据不太敏感，算法也比较简单，常用于文本分类。    分类准确度高，速度快缺点：    由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好&quot;&quot;&quot;from sklearn.datasets import fetch_20newsgroups    # 获取数据集from sklearn.model_selection import train_test_split    # 数据集划分from sklearn.model_selection import GridSearchCV    # 模型优化from sklearn.naive_bayes import MultinomialNB    # 朴素贝叶斯from sklearn.feature_extraction.text import TfidfVectorizer    # 文本特征抽取def nb_news():    # 1. 获取数据    news &#x3D; fetch_20newsgroups(subset&#x3D;&quot;all&quot;)    # 2.划分数据集    x_train, x_test, y_train, y_test &#x3D; train_test_split(news.data, news.target)    transfer &#x3D; TfidfVectorizer()    # 标准化    x_train &#x3D; transfer.fit_transform(x_train)    x_test &#x3D; transfer.transform(x_test)    estimator &#x3D; MultinomialNB()    # 模型优化    param_dict &#x3D; &#123;&quot;alpha&quot;: [1, 3, 5, 7, 9, 11]&#125;    estimator &#x3D; GridSearchCV(estimator, param_grid&#x3D;param_dict, cv&#x3D;10)    estimator.fit(x_train, y_train)    # 5. 模型评估    # 方法1：直接比对真实值和预测值    y_predict &#x3D; estimator.predict(x_test)    print(&quot;y_predict:\n&quot;, y_predict)    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)    # 方法2：计算准确率    score &#x3D; estimator.score(x_test, y_test)    print(&quot;准确率为：\n&quot;, score)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    nb_news()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict: [ 9  4 17 ...  8  2  0]直接比对真实值和预测值: [ True  True  True ...  True  True  True]准确率为： 0.8548387096774194<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="API-6"><a href="#API-6" class="headerlink" title="API"></a>API</h4><ul><li>class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)<ul><li>决策树分类器</li><li>criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’</li><li>max_depth:树的深度大小</li><li>random_state:随机数种子</li></ul></li><li>其中会有些超参数：max_depth:树的深度大小<ul><li>其它超参数我们会结合随机森林讲解</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_iris    # 数据集from sklearn.model_selection import train_test_split    # 数据集划分from sklearn.tree import DecisionTreeClassifier    # 决策树def decision_iris():    &quot;&quot;&quot;    用决策树对鸢尾花进行分类    优点：可视化 - 可解释能力强    缺点：如果数据很多还不设置树的深度这样可能会产生过拟合。    &quot;&quot;&quot;    # 1. 获取数据集    iris &#x3D; load_iris()    # 2. 划分数据集    x_train, x_test, y_train, y_test &#x3D; train_test_split(iris.data, iris.target)    # 3. 决策树预估器    # 参数max_depth:树的深度大小 如果数据集大则树的深度也大这样可能会导致准确率降低    # 所以如果有必要的话可以设置树的深度来提高准确率    estimator &#x3D; DecisionTreeClassifier(criterion&#x3D;&quot;entropy&quot;)    estimator.fit(x_train, y_train)    # 4. 模型评估    # 方法1：直接比对真实值和预测值    y_predict &#x3D; estimator.predict(x_test)    print(&quot;y_predict:\n&quot;, y_predict)    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)    # 方法2：计算准确率    score &#x3D; estimator.score(x_test, y_test)    print(&quot;准确率为：\n&quot;, score)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    decision_iris()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict: [0 2 1 0 1 0 0 0 2 1 2 1 2 2 1 1 1 0 0 1 2 2 0 0 2 1 0 2 1 2 1 0 1 1 0 1 1 1]直接比对真实值和预测值: [ True  True  True  True  True  True  True  True  True  True  True  True  True  True  True False  True  True  True False  True  True  True  True  True  True  True  True False False  True  True  True  True  True False  True  True]准确率为： 0.868421052631579<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><h4 id="随机森林原理过程"><a href="#随机森林原理过程" class="headerlink" title="随机森林原理过程"></a>随机森林原理过程</h4><p>学习算法根据下列算法而建造每棵树：</p><ul><li>用N来表示训练用例（样本）的个数，M表示特征数目。<ul><li>1、一次随机选出一个样本，重复N次， （有可能出现重复的样本）</li><li>2、随机去选出m个特征, m &lt;&lt;M，建立决策树</li></ul></li><li>采取bootstrap抽样</li></ul><h4 id="为什么采用BootStrap抽样"><a href="#为什么采用BootStrap抽样" class="headerlink" title="为什么采用BootStrap抽样"></a>为什么采用BootStrap抽样</h4><ul><li>为什么要随机抽样训练集？　　<ul><li>如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的</li></ul></li><li>为什么要有放回地抽样？<ul><li>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。</li></ul></li></ul><h4 id="API-7"><a href="#API-7" class="headerlink" title="API"></a>API</h4><ul><li><p>class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)</p><ul><li>随机森林分类器</li><li>n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</li><li>criteria：string，可选（default =“gini”）分割特征的测量方法</li><li>max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</li><li>max_features=”auto”,每个决策树的最大特征数量<ul><li>If “auto”, then <code>max_features=sqrt(n_features)</code>.</li><li>If “sqrt”, then <code>max_features=sqrt(n_features)</code> (same as “auto”).</li><li>If “log2”, then <code>max_features=log2(n_features)</code>.</li><li>If None, then <code>max_features=n_features</code>.</li></ul></li><li>bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样</li><li>min_samples_split:节点划分最少样本数</li><li>min_samples_leaf:叶子节点的最小样本数</li></ul></li><li><p>超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_iris    # 数据集from sklearn.model_selection import train_test_split    # 数据集划分from sklearn.ensemble import RandomForestClassifier    # 随机森林from sklearn.model_selection import GridSearchCV    # 模型优化def iris_demo():    # 加载数据集    iris &#x3D; load_iris()    # 数据集划分    x_train, x_test, y_train, y_test &#x3D; train_test_split(iris.data, iris.target)    # 随机森林    estimator &#x3D; RandomForestClassifier()    # 网格化搜索    param_dict &#x3D; &#123;&quot;n_estimators&quot;: [10, 20, 30, 40, 50, 100]&#125;    estimator &#x3D; GridSearchCV(estimator, param_grid&#x3D;param_dict, cv&#x3D;10)    # 训练    estimator.fit(x_train, y_train)    # 模型评估    # 方法1：直接比对真实值和预测值    y_predict &#x3D; estimator.predict(x_test)    print(&quot;y_predict:\n&quot;, y_predict)    print(&quot;直接比对真实值和预测值:\n&quot;, y_test &#x3D;&#x3D; y_predict)    # 方法2：计算准确率    score &#x3D; estimator.score(x_test, y_test)    print(&quot;准确率为：\n&quot;, score)    # 最佳参数：best_params_    print(&quot;最佳参数：\n&quot;, estimator.best_params_)    # 最佳结果：best_score_    print(&quot;最佳结果：\n&quot;, estimator.best_score_)    # 最佳估计器：best_estimator_    print(&quot;最佳估计器:\n&quot;, estimator.best_estimator_)    # 交叉验证结果：cv_results_    print(&quot;交叉验证结果:\n&quot;, estimator.cv_results_)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    iris_demo()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_predict: [0 1 2 2 1 2 2 1 2 2 0 0 0 0 0 0 1 2 0 2 1 2 2 0 0 0 1 2 2 2 1 1 1 1 0 2 1 2]直接比对真实值和预测值: [ True  True  True  True  True  True  True  True  True  True  True  True    True  True  True  True  True  True  True  True  True  True  True  True     True  True  True  True  True  True False  True  True  True  True  True     True  True]准确率为： 0.9736842105263158最佳参数： &#123;&#39;n_estimators&#39;: 30&#125;最佳结果： 0.9553030303030303最佳估计器: RandomForestClassifier(n_estimators&#x3D;30)交叉验证结果: &#123;&#39;mean_fit_time&#39;: array([0.01545861, 0.0299505 , 0.04111712, 0.05707047, 0.06866693,       0.145067  ]), &#39;std_fit_time&#39;: array([0.00215007, 0.00540963, 0.00032881, 0.00359371, 0.00326092,       0.0084516 ]), &#39;mean_score_time&#39;: array([0.00159588, 0.0027936 , 0.00379615, 0.00488796, 0.00544808,       0.01146989]), &#39;std_score_time&#39;: array([0.00048815, 0.00040718, 0.00039285, 0.00054974, 0.00046851,       0.00091972]), &#39;param_n_estimators&#39;: masked_array(data&#x3D;[10, 20, 30, 40, 50, 100],             mask&#x3D;[False, False, False, False, False, False],       fill_value&#x3D;&#39;?&#39;,            dtype&#x3D;object), &#39;params&#39;: [&#123;&#39;n_estimators&#39;: 10&#125;, &#123;&#39;n_estimators&#39;: 20&#125;, &#123;&#39;n_estimators&#39;: 30&#125;, &#123;&#39;n_estimators&#39;: 40&#125;, &#123;&#39;n_estimators&#39;: 50&#125;, &#123;&#39;n_estimators&#39;: 100&#125;], &#39;split0_test_score&#39;: array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,       0.91666667]), &#39;split1_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split2_test_score&#39;: array([0.90909091, 1.        , 1.        , 1.        , 1.        ,       1.        ]), &#39;split3_test_score&#39;: array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,       0.90909091]), &#39;split4_test_score&#39;: array([0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,       0.81818182]), &#39;split5_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split6_test_score&#39;: array([1., 1., 1., 1., 1., 1.]), &#39;split7_test_score&#39;: array([0.90909091, 0.90909091, 1.        , 0.90909091, 1.        ,       1.        ]), &#39;split8_test_score&#39;: array([1.        , 0.90909091, 1.        , 0.90909091, 1.        ,       1.        ]), &#39;split9_test_score&#39;: array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,       0.90909091]), &#39;mean_test_score&#39;: array([0.93712121, 0.93712121, 0.95530303, 0.93712121, 0.95530303,       0.95530303]), &#39;std_test_score&#39;: array([0.05789881, 0.05789881, 0.0604591 , 0.05789881, 0.0604591 ,       0.0604591 ]), &#39;rank_test_score&#39;: array([4, 4, 1, 4, 1, 1])&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>在当前所有算法中，具有极好的准确率</li><li>能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维</li><li>能够评估各个特征在分类问题上的重要性</li></ul><h2 id="回归与聚类算法"><a href="#回归与聚类算法" class="headerlink" title="回归与聚类算法"></a>回归与聚类算法</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h4><ul><li>sklearn.linear_model.LinearRegression(fit_intercept=True)<ul><li>通过正规方程优化</li><li>fit_intercept：是否计算偏置</li><li>LinearRegression.coef_：回归系数</li><li>LinearRegression.intercept_：偏置</li></ul></li><li>sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01)<ul><li>SGDRegressor类实现了随机梯度下降学习，它支持不同的<strong>loss函数和正则化惩罚项</strong>来拟合线性回归模型。</li><li>loss:损失类型<ul><li><strong>loss=”squared_loss”: 普通最小二乘法</strong></li></ul></li><li>fit_intercept：是否计算偏置</li><li>learning_rate : string, optional<ul><li>学习率填充</li><li><strong>‘constant’: eta = eta0</strong></li><li><strong>‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]</strong></li><li>‘invscaling’: eta = eta0 / pow(t, power_t)<ul><li><strong>power_t=0.25:存在父类当中</strong></li></ul></li><li><strong>对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</strong></li></ul></li><li>SGDRegressor.coef_：回归系数</li><li>SGDRegressor.intercept_：偏置</li></ul></li></ul><blockquote><p>sklearn提供给我们两种实现的API， 可以根据选择使用</p></blockquote><h4 id="回归性能评估"><a href="#回归性能评估" class="headerlink" title="回归性能评估"></a>回归性能评估</h4><p>sklearn.metrics.mean_squared_error(y_true, y_pred)</p><ul><li>均方误差回归损失</li><li>y_true:真实值</li><li>y_pred:预测值</li><li>return:浮点数结果</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_boston    # 加载数据集from sklearn.model_selection import train_test_split    # 数据集划分from sklearn.preprocessing import StandardScaler    # 特征工程 标准化from sklearn.linear_model import LinearRegression, SGDRegressor    # 正规方程 梯度下降from sklearn.metrics import mean_squared_error    # 回归评估 均方误差（值越小越好）def linear1():    &quot;&quot;&quot;    正规方程的优化方法对波士顿房价进行预测    &quot;&quot;&quot;    boston &#x3D; load_boston()    x_train, x_test, y_train, y_test &#x3D; train_test_split(        boston.data, boston.target, random_state&#x3D;22)    transfer &#x3D; StandardScaler()    x_train &#x3D; transfer.fit_transform(x_train)    x_test &#x3D; transfer.transform(x_test)    estimator &#x3D; LinearRegression()    estimator.fit(x_train, y_train)    print(&quot;正规方程-权重系数为：\n&quot;, estimator.coef_)    print(&quot;正规方程-偏置为：\n&quot;, estimator.intercept_)    y_predict &#x3D; estimator.predict(x_test)    print(&quot;正规方程-预测房价：\n&quot;, y_predict)    error &#x3D; mean_squared_error(y_test, y_predict)    print(&quot;正规方程-均方误差：\n&quot;, error)    return Nonedef linear2():    &quot;&quot;&quot;    梯度下降的优化方法对波士顿房价进行预测    &quot;&quot;&quot;    boston &#x3D; load_boston()    x_train, x_test, y_train, y_test &#x3D; train_test_split(        boston.data, boston.target, random_state&#x3D;22)    transfer &#x3D; StandardScaler()    x_train &#x3D; transfer.fit_transform(x_train)    x_test &#x3D; transfer.transform(x_test)    estimator &#x3D; SGDRegressor()    estimator.fit(x_train, y_train)    print(&quot;梯度下降-权重系数为：\n&quot;, estimator.coef_)    print(&quot;梯度下降-偏置为：\n&quot;, estimator.intercept_)    y_predict &#x3D; estimator.predict(x_test)    print(&quot;梯度下降-预测房价：\n&quot;, y_predict)    error &#x3D; mean_squared_error(y_test, y_predict)    print(&quot;梯度下降-均方误差：\n&quot;, error)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    linear1()    linear2()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">正规方程-权重系数为： [-0.64817766  1.14673408 -0.05949444  0.74216553 -1.95515269  2.70902585 -0.07737374 -3.29889391  2.50267196 -1.85679269 -1.75044624  0.87341624  -3.91336869]正规方程-偏置为： 22.62137203166228正规方程-预测房价： [28.22944896 31.5122308  21.11612841 32.6663189  20.0023467  19.07315705 21.09772798 19.61400153 19.61907059 32.87611987 20.97911561 27.52898011  15.54701758 19.78630176 36.88641203 18.81202132  9.35912225 18.49452615  30.66499315 24.30184448 19.08220837 34.11391208 29.81386585 17.51775647  34.91026707 26.54967053 34.71035391 27.4268996  19.09095832 14.92742976  30.86877936 15.88271775 37.17548808  7.72101675 16.24074861 17.19211608   7.42140081 20.0098852  40.58481466 28.93190595 25.25404307 17.74970308  38.76446932  6.87996052 21.80450956 25.29110265 20.427491   20.4698034   17.25330064 26.12442519  8.48268143 27.50871869 30.58284841 16.56039764   9.38919181 35.54434377 32.29801978 21.81298945 17.60263689 22.0804256   23.49262401 24.10617033 20.1346492  38.5268066  24.58319594 19.78072415  13.93429891  6.75507808 42.03759064 21.9215625  16.91352899 22.58327744  40.76440704 21.3998946  36.89912238 27.19273661 20.97945544 20.37925063  25.3536439  22.18729123 31.13342301 20.39451125 23.99224334 31.54729547  26.74581308 20.90199941 29.08225233 21.98331503 26.29101202 20.17329401  25.49225305 24.09171045 19.90739221 16.35154974 15.25184758 18.40766132  24.83797801 16.61703662 20.89470344 26.70854061 20.7591883  17.88403312  24.28656105 23.37651493 21.64202047 36.81476219 15.86570054 21.42338732  32.81366203 33.74086414 20.61688336 26.88191023 22.65739323 17.35731771  21.67699248 21.65034728 27.66728556 25.04691687 23.73976625 14.6649641   15.17700342  3.81620663 29.18194848 20.68544417 22.32934783 28.01568563  28.58237108]正规方程-均方误差： 20.6275137630954梯度下降-权重系数为： [-0.52844184  0.94742828 -0.43896185  0.80011052 -1.70094588  2.83803954 -0.15173308 -3.15314827  1.64459185 -0.92057374 -1.72092516  0.85822999 -3.89079621]梯度下降-偏置为： [22.62848948]梯度下降-预测房价： [28.32292485 31.65691921 21.4754174  32.74532796 20.21502727 19.05803274 21.38012862 19.40598106 19.65928004 32.83684983 21.37788237 27.27547404 15.58434744 19.9543843  37.052139   18.66414622  9.64694632 18.60831548 30.78911441 24.28055663 19.04506654 34.16527664 29.53666782 17.39899068 34.86763757 26.51961514 34.42685759 27.38498522 19.1234001  15.69443705 30.90416755 14.4629764  37.60886681  8.70471496 16.38818819 16.8388242  7.70173349 19.7726613  40.59036302 29.18080616 25.25730687 17.82216674 39.37499484  6.67884528 21.54796917 25.04507005 20.88378703 20.64882236 17.03224546 26.32629603  9.6471961  27.18516688 30.67565735 16.69940767  9.58964556 35.55768574 31.59626177 22.93550937 17.57965025 21.83043993 23.62551177 23.94527301 20.3319314  38.24324246 25.72953353 19.68070738 14.15726975  6.66109511 42.53435973 21.82504626 16.74834226 22.55184196 41.01385643 21.71857114 36.98704919 27.16545173 21.83161702 20.78576019 25.30987444 23.77003724 31.53599899 20.19287298 24.00693276 31.56432429 27.2645538  20.86527222 29.1081147  21.95286144 26.74924411 18.76781794 25.26458575 24.01985914 19.93042174 17.68510263 15.50813547 18.27522698 24.59223379 16.73277678 20.66946955 26.79190915 20.72655782 17.97463708 24.13232421 23.25410362 20.27914227 36.64368885 15.98551815 22.46864296 32.71528599 33.73484378 20.54324486 25.9984172  23.32481418 17.74521574 21.46651544 21.79381137 27.55126895 25.28602934 23.65813339 14.43211265 15.64622064  3.64461024 29.25332792 20.65259117 22.31013756 28.06226938 28.3763949 ]梯度下降-均方误差： 21.168264510208296<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：均方误差越小表示效果越好。</p><h3 id="岭回归—线性回归的改进"><a href="#岭回归—线性回归的改进" class="headerlink" title="岭回归—线性回归的改进"></a>岭回归—线性回归的改进</h3><h4 id="API-8"><a href="#API-8" class="headerlink" title="API"></a>API</h4><p>sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=”auto”, normalize=False)</p><ul><li>具有l2正则化的线性回归</li><li>alpha:正则化力度，也叫 λ<ul><li><strong>λ取值：0<del>1 1</del>10</strong></li></ul></li><li>solver:会根据数据自动选择优化方法<ul><li><strong>sag:如果数据集、特征都比较大，选择该随机梯度下降优化</strong></li></ul></li><li>normalize:数据是否进行标准化<ul><li>normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</li></ul></li><li>Ridge.coef_:回归权重</li><li>Ridge.intercept_:回归偏置</li></ul><p><strong>Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</strong></p><ul><li>sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)<ul><li>具有l2正则化的线性回归，可以进行交叉验证</li><li>coef_:回归系数</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_boston  # 加载数据集from sklearn.linear_model import Ridge  # 岭回归from sklearn.metrics import mean_squared_error  # 回归评估 均方误差（值越小越好）from sklearn.model_selection import train_test_split  # 数据集划分from sklearn.preprocessing import StandardScaler  # 特征工程 标准化def linear3():    # 加载数据集    boston &#x3D; load_boston()    # 数据集划分    x_train, x_test, y_train, y_test &#x3D; train_test_split(        boston.data, boston.target, random_state&#x3D;22)    # 数据集标准化    transfer &#x3D; StandardScaler()    x_train &#x3D; transfer.fit_transform(x_train)    x_test &#x3D; transfer.transform(x_test)    # 预估器    estimator &#x3D; Ridge()    estimator.fit(x_train, y_train)    # 得出模型    print(&quot;岭回归-权重系数为：\n&quot;, estimator.coef_)    print(&quot;岭回归-偏置为：\n&quot;, estimator.intercept_)    # 模型评估    y_predict &#x3D; estimator.predict(x_test)    print(&quot;岭回归-预测房价：\n&quot;, y_predict)    error &#x3D; mean_squared_error(y_test, y_predict)    print(&quot;岭回归-均方误差：\n&quot;, error)    return Noneif __name__ &#x3D;&#x3D; &quot;__main__&quot;:    linear3()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">岭回归-权重系数为： [-0.63591916  1.12109181 -0.09319611  0.74628129 -1.91888749  2.71927719 -0.08590464 -3.25882705  2.41315949 -1.76930347 -1.74279405  0.87205004  -3.89758657]岭回归-偏置为： 22.62137203166228岭回归-预测房价： [28.22119941 31.49858594 21.14690941 32.64962343 20.03976087 19.07187629 21.11827061 19.61935024 19.64669848 32.83666525 21.01034708 27.47939935 15.55875601 19.80406014 36.86415472 18.79442579  9.42343608 18.5205955 30.67129766 24.30659711 19.07820077 34.08772738 29.77396117 17.50394928 34.87750492 26.52508961 34.65566473 27.42939944 19.08639183 15.04854291 30.84974343 15.76894723 37.18814441  7.81864035 16.27847433 17.15510852  7.46590141 19.98474662 40.55565604 28.96103939 25.25570196 17.7598197 38.78171653  6.87935126 21.76805062 25.25888823 20.47319256 20.48808719 17.24949519 26.11755181  8.61005188 27.47070495 30.57806886 16.57080888  9.42312214 35.50731907 32.20467352 21.93128073 17.62011278 22.08454636 23.50121152 24.08248876 20.16840581 38.47001591 24.69276673 19.7638548 13.96547058  6.76070715 42.04033544 21.9237625  16.88030656 22.60637682 40.74664535 21.44631815 36.86936185 27.17135794 21.09470367 20.40689317 25.35934079 22.35676321 31.1513028  20.39303322 23.99948991 31.54251155 26.77734347 20.89368871 29.05880401 22.00850263 26.31965286 20.04852734 25.46476799 24.08084537 19.90846889 16.47030743 15.27936372 18.39475348 24.80822272 16.62280764 20.86393724 26.70418608 20.74534996 17.89544942 24.25949423 23.35743497 21.51817773 36.76202304 15.90293344 21.52915882 32.78684766 33.68666117 20.61700911 26.78345059 22.72685584 17.40478038 21.67136433 21.6912557  27.66684993 25.08825085 23.72539867 14.64260535 15.21105331  3.81916568 29.16662813 20.67913144 22.33386579 28.01241753 28.531445  ]岭回归-均方误差： 20.65644821435496<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="逻辑回归与二分类—分类算法"><a href="#逻辑回归与二分类—分类算法" class="headerlink" title="逻辑回归与二分类—分类算法"></a>逻辑回归与二分类—分类算法</h3><h4 id="逻辑回归API"><a href="#逻辑回归API" class="headerlink" title="逻辑回归API"></a>逻辑回归API</h4><ul><li>sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)<ul><li>solver:优化求解方式（默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数）<ul><li>sag：根据数据集自动选择，随机平均梯度下降</li></ul></li><li>penalty：正则化的种类</li><li>C：正则化力度</li></ul></li></ul><blockquote><p><strong>默认将类别数量少的当做正例</strong></p></blockquote><p><strong>LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)</strong></p><h4 id="分类评估报告API"><a href="#分类评估报告API" class="headerlink" title="分类评估报告API"></a>分类评估报告API</h4><ul><li>sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )</li><li><ul><li>y_true：真实目标值</li><li>y_pred：估计器预测目标值</li><li>labels:指定类别对应的数字</li><li>target_names：目标类别名称</li><li>return：每个类别精确率与召回率</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npimport pandas as pdfrom sklearn.linear_model import LogisticRegression  # 逻辑回归from sklearn.metrics import classification_report, roc_auc_score    # 分类评估from sklearn.model_selection import train_test_split  # 数据集划分from sklearn.preprocessing import StandardScaler  # 特征工程 标准化if __name__ &#x3D;&#x3D; &quot;__main__&quot;:    # 加载数据    column_name &#x3D; [&#39;Sample code number&#39;, &#39;Clump Thickness&#39;,                   &#39;Uniformity of Cell Size&#39;, &#39;Uniformity of Cell Shape&#39;,                   &#39;Marginal Adhesion&#39;, &#39;Single Epithelial Cell Size&#39;,                   &#39;Bare Nuclei&#39;, &#39;Bland Chromatin&#39;,                   &#39;Normal Nucleoli&#39;, &#39;Mitoses&#39;, &#39;Class&#39;]    path &#x3D; r&quot;D:\痛苦の资料\不会真的有人学编程吧\机器学习\黑马程序员\自己的整理\day3\breast-cancer-wisconsin.data&quot;    data &#x3D; pd.read_csv(path, names&#x3D;column_name)    # 数据处理  因为数据中有缺失值，缺失的地方使用 ? 替代，目标是删掉缺失值    data &#x3D; data.replace(to_replace&#x3D;&#39;?&#39;, value&#x3D;np.nan)    data.dropna(inplace&#x3D;True)    # 数据处理  获取特征值和目标值    x &#x3D; data.iloc[:, 1:-1]    y &#x3D; data[&quot;Class&quot;]    # 数据集划分    x_train, x_test, y_train, y_test &#x3D; train_test_split(x, y)    # 数据标准化    transfer &#x3D; StandardScaler()    x_train &#x3D; transfer.fit_transform(x_train)    x_test &#x3D; transfer.transform(x_test)    # 预估器    estimator &#x3D; LogisticRegression()    estimator.fit(x_train, y_train)    # 模型    print(&quot;逻辑回归-权重系数为：\n&quot;, estimator.coef_)    print(&quot;逻辑回归-偏置为：\n&quot;, estimator.intercept_)    y_predict &#x3D; estimator.predict(x_test)    print(&quot;y_predict：\n&quot;, y_predict)    print(&quot;直接比对真实值和预测值：\n&quot;, y_test &#x3D;&#x3D; y_predict)    score &#x3D; estimator.score(x_test, y_test)    print(&quot;准确率：\n&quot;, score)    # 查看精确率、召回率、F1-score    report &#x3D; classification_report(y_test, y_predict, labels&#x3D;[                                   2, 4], target_names&#x3D;[&#39;良性&#39;, &#39;恶性&#39;])    # y_true:每个样本的真实类别，必须为0(反例),1(正例)标记    # 将y_test 转换成 0 1    y_true &#x3D; np.where(y_test &gt; 3, 1, 0)    print(roc_auc_score(y_true, y_predict))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">逻辑回归-权重系数为： [[1.51911637 0.06401522 0.69614721 0.76445915 0.50264081 1.37783828  1.13432057 0.6161716  0.85294448]]逻辑回归-偏置为： [-0.90973535]y_predict： [2 4 2 4 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 4 2 4 2 4 4 2 2 4 4 4 4 4 2 2 2 4 2 4 4 2 2 2 2 4 2 2 2 4 2 4 4 2 2 2 2 2 2 2 2 4 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 4 4 4 2 2 2 2 4 4 2 4 4 2 4 2 4 4 4 2 2 2 2 2 2 4 2 4 2 2 2 2 2 4 2 4 4 2 4 2 2 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 4 2 4 2 2 2 2 2 2 2 2 4 4 2]直接比对真实值和预测值： 137    True506    True578    True100    True600    True       ...426    True390    True336    True289    True444    TrueName: Class, Length: 171, dtype: bool准确率： 0.96491228070175440.9585488041370394<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="无监督学习—K-means算法"><a href="#无监督学习—K-means算法" class="headerlink" title="无监督学习—K-means算法"></a>无监督学习—K-means算法</h3><h4 id="K-meansAPI"><a href="#K-meansAPI" class="headerlink" title="K-meansAPI"></a>K-meansAPI</h4><ul><li>sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)<ul><li>k-means聚类</li><li>n_clusters:开始的聚类中心数量</li><li>init:初始化方法，默认为’k-means ++’</li><li>labels_:默认标记的类型，可以和真实值比较（不是值比较）</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV颜色识别 物体追踪</title>
      <link href="posts/7.html"/>
      <url>posts/7.html</url>
      
        <content type="html"><![CDATA[<h1 id="OpenCV颜色识别-物体追踪"><a href="#OpenCV颜色识别-物体追踪" class="headerlink" title="OpenCV颜色识别 物体追踪"></a>OpenCV颜色识别 物体追踪</h1><a id="more"></a><p>对于颜色识别和<code>imutils</code>包的用法请浏览我得另一篇博客：<a href="https://lightningleader.github.io/posts/6.html">OpenCV学习笔记</a></p><h2 id="代码原理"><a href="#代码原理" class="headerlink" title="代码原理"></a>代码原理</h2><p>这是个比较简单的代码。代码实现的就是简单的物体追踪，将物体用方框框出。</p><p>简单来讲就是先进行颜色识别，正确识别到物体后获取物体的外接矩形再画出外接矩形即可。</p><p>详细的解释可以看代码注释，应该是容易理解的。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import cv2import numpy as npimport imutilsfrom imutils import contours# 颜色阈值lower &#x3D; np.array([130, 62, 72])upper &#x3D; np.array([170, 255, 148])# 内核kernel &#x3D; np.ones((5, 5), np.uint8)# 打开摄像头vc &#x3D; cv2.VideoCapture(0)if vc.isOpened():    flag, frame &#x3D; vc.read()    # 翻转图像    # 这一步可以忽略，博主的摄像头是反着的    # 所以加上这句话可以让摄像头的图像正过来    frame &#x3D; imutils.rotate(frame, 180)    cv2.imshow(&quot;frame&quot;, frame)else:    flag &#x3D; Falsewhile flag:    flag, frame &#x3D; vc.read()    # 翻转图像    frame &#x3D; imutils.rotate(frame, 180)    draw_frame &#x3D; frame.copy()    if frame is None:        break    if flag is True:        &#39;&#39;&#39;下面对摄像头读取到的图像进行处理，这个步骤是比较重要的&#39;&#39;&#39;        # 转换颜色空间HSV        frame_hsv &#x3D; cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)        # 颜色识别        img &#x3D; cv2.inRange(frame_hsv, lower, upper)        # 膨胀操作        dilation &#x3D; cv2.dilate(img, kernel, iterations&#x3D;1)        # 闭操作        closing &#x3D; cv2.morphologyEx(dilation, cv2.MORPH_CLOSE, kernel)        # 高斯滤波        closing &#x3D; cv2.GaussianBlur(closing, (5, 5), 0)        # 边缘检测        edges &#x3D; cv2.Canny(closing, 10, 20)        &#39;&#39;&#39;上面进行那么多操作就是为了得到更好的目标图形，具体效果因环境而异&#39;&#39;&#39;        # 寻找轮廓        cnts, _ &#x3D; cv2.findContours(            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)        # 判断轮廓数量也就是判断是否寻找到轮廓，如果没有找到轮廓就不继续进行操作        if len(cnts) &gt; 0:            # 存放轮廓面积的列表            s &#x3D; []            # 存放最大轮廓的索引            max_index &#x3D; 0            # 获得排序后的轮廓列表以及每个轮廓对应的外接矩形            (cnts, boundingRects) &#x3D; contours.sort_contours(cnts)            # 寻找面积最大的轮廓的索引            for cnt in cnts:                s.append(cv2.contourArea(cnt))            max_index &#x3D; s.index(max(s))            # 根据面积最大轮廓的索引找到它的外接矩形的信息            (x, y, w, h) &#x3D; boundingRects[max_index]            # 画矩形            frame_out &#x3D; cv2.rectangle(                         draw_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)        cv2.imshow(&quot;frame&quot;, draw_frame)        if cv2.waitKey(10) &#x3D;&#x3D; 27:            breakvc.release()cv2.destroyAllWindows()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>具体解释都在代码注释里面了，下面我要提一些注意的地方。</p><ol><li>获得过图像之后可能需要进行一系列的形态学操作，这样可以让获得的图像更加接近目的图像。这些操作虽然不是必需的但如果正常取得的图像效果不好的时候可以加上。</li><li>函数<code>contours.sort_contours</code> 和 <code>imutils.rotate</code>是<code>imutils</code>包里面的，使用起来比较方便，具体用法可以看博主的另一篇博客：<a href="https://lightningleader.github.io/posts/6.html">OpenCV学习笔记</a>。外接矩形也可以通过<code>cv2.boundingRect</code>函数获得。</li><li>代码中博主是以轮廓的最大面积为条件画的图，但这个条件应该根据自己想要的效果而定。这里除了寻找最大面积以外还可以寻找最大周长、最长的边之类的。</li></ol><h2 id="最终效果图"><a href="#最终效果图" class="headerlink" title="最终效果图"></a>最终效果图</h2><p><img src="/posts/7/1.jpg" alt="最终效果图"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 图像处理 </tag>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV学习笔记</title>
      <link href="posts/6.html"/>
      <url>posts/6.html</url>
      
        <content type="html"><![CDATA[<h1 id="OpenCV学习笔记"><a href="#OpenCV学习笔记" class="headerlink" title="OpenCV学习笔记"></a>OpenCV学习笔记</h1><a id="more"></a><h2 id="图像翻转"><a href="#图像翻转" class="headerlink" title="图像翻转"></a>图像翻转</h2><p>使用Python的一个包，imutils。使用下面的指令可以安装。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install imutils<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>imutils包的Github地址：<a href="https://github.com/jrosebr1/imutils">https://github.com/jrosebr1/imutils</a></p><p>CSDN镜像：<a href="https://codechina.csdn.net/mirrors/jrosebr1/imutils">https://codechina.csdn.net/mirrors/jrosebr1/imutils</a></p><p>可以在上面这个地址里面学习更多的使用方式。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import cv2import imutils&#39;&#39;&#39;imutils.rotate第一个参数是翻转的图像，第二个参数的翻转角度函数还提供翻转中心的设置，但默认就是中心翻转。&#39;&#39;&#39;vc &#x3D; cv2.VideoCapture(0)if vc.isOpened():    flag, frame &#x3D; vc.read()    img &#x3D; imutils.rotate(frame, 180)    # 图像翻转     cv2.imshow(&quot;frame&quot;, img)else:    flag &#x3D; Falsewhile flag:    flag, frame &#x3D; vc.read()    if frame is None:        break    if flag is True:        img &#x3D; imutils.rotate(frame, 180)    # 图像翻转        cv2.imshow(&quot;frame&quot;, img)        if cv2.waitKey(10) &#x3D;&#x3D; 27:            breakvc.release()cv2.destroyAllWindows()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样写的话，最后的输出图像就是翻转180度的。</p><p>imutils包里还有其他好用的函数，resizing、4-point Perspective Transform、Sorting Contours等等。</p><h2 id="图像轮廓排序"><a href="#图像轮廓排序" class="headerlink" title="图像轮廓排序"></a>图像轮廓排序</h2><p>这个效果同样也是依靠imutils包完成。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from imutils import contoursimport cv2&#39;&#39;&#39;contours.sort_contours可选排序方式：&quot;left-to-right&quot;, &quot;right-to-left&quot;, &quot;top-to-bottom&quot;, &quot;bottom-to-top&quot;返回值为轮廓和外接矩形contours.label_contourcontours包内自带的画轮廓的函数，可以直接用，然后可以在图片上标出轮廓序号也可以直接使用cv2.drawContours直接画轮廓&#39;&#39;&#39;img &#x3D; cv2.imread(r&quot;D:\opencv-workspace\Opencv\test17--VScode\shapes.png&quot;)draw_img &#x3D; img.copy()img_rect &#x3D; img.copy()gray &#x3D; cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)img &#x3D; cv2.Canny(gray, 10, 20)    # Canny边缘检测cnts, hierarchy &#x3D; cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)    # 获得轮廓(cnts, boundingBoxes) &#x3D; contours.sort_contours(cnts, &quot;top-to-bottom&quot;)    # 对轮廓进行排序处理for (i, c) in enumerate(cnts):    sortedImage &#x3D; contours.label_contour(draw_img, c, i, color&#x3D;(240, 0, 159))# img_out &#x3D; cv2.drawContours(draw_img, cnts, -1, (240, 0, 159), 2)# 根据boundingBoxes画外接矩形for (x, y, w, h) in boundingBoxes:    img_rect &#x3D; cv2.rectangle(img_rect, (x, y), (x+w, y+h), (240, 0, 159), 2)cv2.imshow(&quot;top-to-bottom&quot;, sortedImage)cv2.imshow(&quot;rect&quot;, img_rect)cv2.waitKey(0)cv2.destroyAllWindows()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/posts/6/1.jpg" alt="处理效果"></p><h2 id="颜色识别"><a href="#颜色识别" class="headerlink" title="颜色识别"></a>颜色识别</h2><h3 id="基础颜色识别"><a href="#基础颜色识别" class="headerlink" title="基础颜色识别"></a>基础颜色识别</h3><p>颜色识别是在HSV空间内进行的，因此在使用之前先进行颜色空间的转换。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">&#39;&#39;&#39;使用下面这个函数进行转换，第一个参数填写要转换的图片，第二个参数填写cv2.COLOR_BGR2HSV&#39;&#39;&#39;cv2.cvtColor<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">import cv2import numpy as np&#39;&#39;&#39;cv2.inRange函数很简单，参数有三个第一个参数：hsv指的是原图第二个参数：lower_red指的是图像中低于这个lower_red的值，图像值变为0第三个参数：upper_red指的是图像中高于这个upper_red的值，图像值变为0而在lower_red～upper_red之间的值变成255&#39;&#39;&#39;# 阈值lower_green &#x3D; np.array([50, 255, 255])upper_green &#x3D; np.array([70, 255, 255])img &#x3D; cv2.imread(r&quot;D:\opencv-workspace\Opencv\test16--VScode\photo.jpg&quot;)img_hsv &#x3D; cv2.cvtColor(img, cv2.COLOR_BGR2HSV)mask_green &#x3D; cv2.inRange(img_hsv, lower_green, upper_green)cv2.imshow(&quot;img_or&quot;, mask_green)# 使用下面这个函数能显示原来的颜色。res_green &#x3D; cv2.bitwise_and(img, img, mask&#x3D;mask_green)cv2.imshow(&quot;img&quot;, res_green)cv2.waitKey(0)cv2.destroyAllWindows()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/posts/6/2.jpg" alt="原图"></p><p><img src="/posts/6/4.jpg" alt="mask_green"></p><p><img src="/posts/6/3.jpg" alt="res_green"></p><p>在进行颜色识别时，难免会出现“漏颜色”的现象，也就是会出现没识别全的现象。这个时候可以再对图像进行处理，比如说进行形态学处理，让图像更加饱满之类的。</p><h3 id="根据BGR获取HSV"><a href="#根据BGR获取HSV" class="headerlink" title="根据BGR获取HSV"></a>根据BGR获取HSV</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import cv2color &#x3D; np.uint8([[[193, 189, 147]]])    # 参数填写BGR的值hsv &#x3D; cv2.cvtColor(color, cv2.COLOR_BGR2HSV)print(hsv)    # 打印出来的数值就是对应的HSV值<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>程序运行的结果是</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">[[[ 93  61 193]]]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个就是对应的HSV的值。</p><p>根据之前写的颜色识别，就需要把对应的阈值写出。具体写法就是<strong>保持S和V不变，H加减10。</strong>这样的话就可以写出高低阈值然后应用到颜色识别里面就可以了。</p><h3 id="阈值编辑器"><a href="#阈值编辑器" class="headerlink" title="阈值编辑器"></a>阈值编辑器</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import cv2import numpy as npdef function(x):    lowH &#x3D; cv2.getTrackbarPos(&quot;lowH&quot;, &quot;img_666&quot;)    lowS &#x3D; cv2.getTrackbarPos(&quot;lowS&quot;, &quot;img_666&quot;)    lowV &#x3D; cv2.getTrackbarPos(&quot;lowV&quot;, &quot;img_666&quot;)    HighH &#x3D; cv2.getTrackbarPos(&quot;HighH&quot;, &quot;img_666&quot;)    HighS &#x3D; cv2.getTrackbarPos(&quot;HighS&quot;, &quot;img_666&quot;)    HighV &#x3D; cv2.getTrackbarPos(&quot;HighV&quot;, &quot;img_666&quot;)    # print(lowH, lowS, lowV, HighH, HighS, HighV)    lower &#x3D; np.uint8([lowH, lowS, lowV])    upper &#x3D; np.uint8([HighH, HighS, HighV])    mask &#x3D; cv2.inRange(img_hsv, lower, upper)    res &#x3D; cv2.bitwise_and(img, img, mask&#x3D;mask)    cv2.imshow(&quot;img&quot;, res)img &#x3D; cv2.imread(r&quot;D:\opencv-workspace\Opencv\test16--VScode\test.jpg&quot;)img_hsv &#x3D; cv2.cvtColor(img, cv2.COLOR_BGR2HSV)cv2.namedWindow(&quot;img_666&quot;)cv2.createTrackbar(&quot;lowH&quot;, &quot;img_666&quot;, 0, 179, function)cv2.createTrackbar(&quot;lowS&quot;, &quot;img_666&quot;, 0, 255, function)cv2.createTrackbar(&quot;lowV&quot;, &quot;img_666&quot;, 0, 255, function)cv2.createTrackbar(&quot;HighH&quot;, &quot;img_666&quot;, 0, 179, function)cv2.createTrackbar(&quot;HighS&quot;, &quot;img_666&quot;, 0, 255, function)cv2.createTrackbar(&quot;HighV&quot;, &quot;img_666&quot;, 0, 255, function)cv2.imshow(&quot;img&quot;, img)cv2.waitKey(0)cv2.destroyAllWindows()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>写了一个比较垃圾的阈值编辑器。。。就不多解释了。。</p><p><img src="/posts/6/5.jpg" alt="阈值编辑器"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
            <tag> 图像处理 </tag>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>树莓派串口通信 USB串口通信</title>
      <link href="posts/5.html"/>
      <url>posts/5.html</url>
      
        <content type="html"><![CDATA[<h1 id="树莓派串口通信-USB串口通信"><a href="#树莓派串口通信-USB串口通信" class="headerlink" title="树莓派串口通信 USB串口通信"></a>树莓派串口通信 USB串口通信</h1><a id="more"></a><p>如果不知道树莓派怎么使用USB串口通信的，欢迎浏览我的另一篇博客文章：<a href="https://lightningleader.github.io/posts/4.html">树莓派使用USB串口通信 CH340</a></p><p>这篇文章就以USB串口为例，简单的说下串口通信时常用的几个命令。注意：代码均为Python编写。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ul><li>树莓派4</li><li>两个CH340</li><li>串口助手</li></ul><h2 id="发送数据"><a href="#发送数据" class="headerlink" title="发送数据"></a>发送数据</h2><ol><li><p>最基础的发送数据方式</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">&#39;&#39;&#39;ser.write(data)    # 发送数据data，data为字节型&#39;&#39;&#39;import serialser &#x3D; serial.Serial(&quot;&#x2F;dev&#x2F;ttyUSB0&quot;, 115200)ser.flushInput()    # 清除缓存ser.write(&quot;LightningMaster\r\n&quot;.encode())    # 发送数据  \r\n可以实现换行  encode()默认是&#39;utf-8&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行这个代码就可以在串口助手上收到数据。</p><p><img src="/posts/5/1.jpg" alt="最基础发送数据"></p></li><li><p>发送中文</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import serialser &#x3D; serial.Serial(&quot;&#x2F;dev&#x2F;ttyUSB0&quot;, 115200)ser.flushInput()    # 清除缓存ser.write(&quot;闪电丶教主\r\n&quot;.encode(&#39;gb2312&#39;))    # 发送数据  \r\n可以实现换行<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/posts/5/2.jpg" alt="发送中文"></p></li><li><p>发送数据包 十六进制</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import serialimport structser &#x3D; serial.Serial(&quot;&#x2F;dev&#x2F;ttyUSB0&quot;, 115200)ser.flushInput()    # 清除缓存pack &#x3D; struct.pack(&#39;BBBB&#39;, 0xaa, 6, 7, 0x55)    # 将数据打包 格式是unsigned charser.write(pack)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/posts/5/3.jpg" alt="发送数据包"></p><p>这个地方使用到了struct.pack，具体有啥作用可以自行百度去看看，我在这就不多说了。这里的作用就是将数据打包，返回格式是unsigned char 的字节串。</p><p>可以使用这个函数给数据加上“头”“尾”之类的东西。</p></li></ol><h2 id="接收数据"><a href="#接收数据" class="headerlink" title="接收数据"></a>接收数据</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">&#39;&#39;&#39;ser.read(num)    # 读取收到的num个字节的数据ser.inWaiting()    # 可以获取还未读出的数据&#39;&#39;&#39;import serialimport structimport timeser &#x3D; serial.Serial(&quot;&#x2F;dev&#x2F;ttyUSB0&quot;, 115200)ser.flushInput()    # 清除缓存while True:    count &#x3D; ser.inWaiting()    # 获取还有多少字符未读    if count !&#x3D; 0:        data &#x3D; ser.read(count)    # 读取数据存到data中        print(data)    # 打印接受到的数据    time.sleep(0.1)    # 系统等待<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行代码，使用串口助手发送数据，可以在树莓派中看到返回值。</p><p><img src="/posts/5/4.jpg" alt="接收数据"></p><p>可以看到树莓派成功接收到了数据。</p><p>如果想把b’ ‘去掉可以使用代码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">print(data.decode(&#39;utf-8&#39;))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/posts/5/5.jpg" alt="去掉b&#39;&#39;"></p><p>接收中文的话需要使用下面的代码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">print(data.decode(&#39;gb2312&#39;))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/posts/5/6.jpg" alt="接收中文"></p><p>具体为什么使用encode和decode，可以自行查阅Python相关资料。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 树莓派 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>树莓派使用USB串口通信 CH340</title>
      <link href="posts/4.html"/>
      <url>posts/4.html</url>
      
        <content type="html"><![CDATA[<h1 id="树莓派使用USB串口通信-CH340"><a href="#树莓派使用USB串口通信-CH340" class="headerlink" title="树莓派使用USB串口通信 CH340"></a>树莓派使用USB串口通信 CH340</h1><a id="more"></a><p>因为需要使用树莓派做自控方向的东西，所以需要使用树莓派串口与各种外设进行通信。使用串口的话个人比较喜欢直接使用USB串口，用起来比较方便。下面就介绍一下怎么使用树莓派的USB串口通信。</p><p>对于不会使用串口通信的hxd可以康康我的这篇文章，里面详细讲解了常用的串口通信的命令：<a href="https://lightningleader.github.io/posts/5.html">树莓派串口通信 USB串口通信</a></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ul><li>树莓派4</li><li>CH340</li><li>串口助手</li></ul><h2 id="树莓派正确识别CH340"><a href="#树莓派正确识别CH340" class="headerlink" title="树莓派正确识别CH340"></a>树莓派正确识别CH340</h2><p>首先正常开启树莓派，不要插入CH340，然后进入树莓派桌面，打开命令行，输入命令lsusb，可以查看本机的USB设备列表，以及USB设备的详细信息。</p><p><img src="/posts/4/1.jpg" alt="输入命令lsusb"></p><p>正常的话，就会像上图那样。</p><p>然后插入CH340，再次运行lsusb命令，这个时候应该会出现下面这个界面。</p><p><img src="/posts/4/2.jpg" alt="再次输入命令lsusb"></p><p>从上图可以明显看出多了一个设备，后面的设备详情也明确表示这个是CH340，这就表明树莓派正确识别到了插入的CH340。</p><p>接着在命令行输入命令 ls -l /dev/tty* </p><p><img src="/posts/4/3.jpg" alt="USB"></p><p>如上图所示，不出意外的话最后一个就是你插入的ch340，然后就记住这个黄色字体即/dev/ttyUSB0。这里的USB0也可能是USB1，这个是不确定的。</p><p>到了这里就代表你的树莓派可以正常识别CH34，下面就是写代码来验证效果了。</p><h2 id="代码实现通信"><a href="#代码实现通信" class="headerlink" title="代码实现通信"></a>代码实现通信</h2><p>这里我的代码是Python的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import serialser &#x3D; serial.Serial(&quot;&#x2F;dev&#x2F;ttyUSB0&quot;, 115200)    # 第一个参数就是上面那个黄色字体  第二个参数填波特率ser.flushInput()    # 清除缓存ser.write(&quot;LightningMaster\r\n&quot;.encode())   # 发送<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>注意：这个教程是默认树莓派已经进行了串口配置的。比如说安装pyserial包，如果没安装的话，可以使用下面的命令进行安装</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip3 install pyserial<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>为了测试，我使用两个CH340，一个连接树莓派另一个连接电脑，这样就可以直接在电脑上通过串口助手看到发送的数据了。</p><p>运行代码后就可以在串口助手上看到树莓派发送的数据了。</p><p><img src="/posts/4/4.jpg" alt="串口助手"></p><p>这样就可以正常使用USB串口进行通信了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 树莓派 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循迹智能小车 循黑线</title>
      <link href="posts/3.html"/>
      <url>posts/3.html</url>
      
        <content type="html"><![CDATA[<h1 id="循迹智能小车-循黑线"><a href="#循迹智能小车-循黑线" class="headerlink" title="循迹智能小车 循黑线"></a>循迹智能小车 循黑线</h1><a id="more"></a><h2 id="硬件菜单"><a href="#硬件菜单" class="headerlink" title="硬件菜单"></a>硬件菜单</h2><ul><li><p>单片机型号：STC16F40K128</p></li><li><p>4路红外循迹模块</p></li><li><p>小车底盘套件（一个底盘、两个TT马达、两个轮胎）</p></li><li><p>12V航模电池</p></li><li><p>降压模块</p></li><li><p>A4950双路电机驱动</p></li><li><p>杜邦线等</p></li></ul><h2 id="硬件使用"><a href="#硬件使用" class="headerlink" title="硬件使用"></a>硬件使用</h2><p><img src="/posts/3/1.png" alt="4路红外循迹模块"></p><ul><li>关于4路红外循迹模块的使用教程，我之前写过一篇博客，里面有详细的介绍。如果有不会的可以先去浏览学习一下。<a href="https://lightningleader.github.io/posts/2.html">跳转至教程</a></li><li>A4950双路电机驱动使用方式不多说了，比较简单。大部分人应该都是用L298N的。</li></ul><h2 id="硬件组装"><a href="#硬件组装" class="headerlink" title="硬件组装"></a>硬件组装</h2><p><img src="/posts/3/2.jpg" alt="循迹小车1"></p><p><img src="/posts/3/3.jpg" alt="循迹小车2"></p><p>硬件组装大概按照上面两张图片组装就行。  </p><ul><li>不过要注意的是红外传感器之间的距离和单个传感器的测量精度。在组装完成后，要通电试一试传感器的工作情况，用它检测黑线，康康是不是能够正常工作。如果无法正常工作需要及时调整传感器的精度或者其他东西。总之要保证传感器能正常检测黑线。</li><li>另一点就是传感器的安装位置。受到传感器数量的限制，所以传感器的安装位置需要仔细考虑。我这里一共有4个传感器所以就像图中那样安装的。中间两个之间留有空隙，空隙的宽度大概为黑线宽度（可以稍微比黑线宽点），其他两个的位置也不要离得太远。具体为什么这样设计再后面讲解。</li></ul><h2 id="程序设计前的理解"><a href="#程序设计前的理解" class="headerlink" title="程序设计前的理解"></a>程序设计前的理解</h2><ul><li>接上面挖下的坑，说一下中间两个传感器之间空隙的问题。</li></ul><p>前提是小车在没有检测到黑线的时候会直线行驶。中间的空隙宽度略微大于黑线宽度，这样就可以满足当黑线处于这个缝隙间的时候传感器不会有反应，小车会直线行驶，在一定程度上满足循线的基础操作。肯定会出现黑线移出这个空隙的情况，这样的话黑线就会被两侧的传感器检测到，然后由单片机做出应对。如果是左侧的传感器检测到黑线（设定小车前进方向为正方向），代表车头右偏了，因此小车需要左转；于此类似，右侧的传感器也是一个道理。理解了这些就可以完成最基本的循线了。</p><ul><li>最外侧两个传感器的作用  </li></ul><p>因为传感器反应速度等等一系列可能的原因会出现黑线在移出中央空隙的时间很短导致传感器没有检测到等的情况，这样的话小车肯定会继续前进而导致无法继续循线。如果在中间两个传感器两侧又安装了两个传感器的话，这样就相当于多了一个保险，如果中间两个没有检测到的话，最外侧的传感器正常工作仍能让小车重新回到循线轨迹上来。不过要注意的是，因为当最外侧的传感器检测到黑线时就意味着车头已经偏的比较严重了，所以要想让车头偏正就需要给更大的“力”，也就是给更大的PWM。</p><h2 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h2><pre class="line-numbers language-C" data-language="C"><code class="language-C">#include &quot;headfile.h&quot;&#x2F;******************** 用于初始化PWM的*******************&#x2F;void All_Init(void)&#123;pwm_init(PWMA_CH1P_P60, 10000, 0);pwm_init(PWMA_CH2P_P62, 10000, 0);pwm_init(PWMA_CH3P_P64, 10000, 0);pwm_init(PWMA_CH4P_P66, 10000, 0);&#125;&#x2F;******************** 车子左转*******************&#x2F;void turn_left(int duty)&#123;pwm_duty(PWMA_CH1P_P60, duty);pwm_duty(PWMA_CH2P_P62, 0);pwm_duty(PWMA_CH3P_P64, 0);pwm_duty(PWMA_CH4P_P66, duty);&#125;&#x2F;******************** 车子右转*******************&#x2F;void turn_right(int duty)&#123;pwm_duty(PWMA_CH1P_P60, 0);pwm_duty(PWMA_CH2P_P62, duty);pwm_duty(PWMA_CH3P_P64, duty);pwm_duty(PWMA_CH4P_P66, 0);&#125;&#x2F;******************** 车子直行*******************&#x2F;void go_straight(void)&#123;pwm_duty(PWMA_CH1P_P60, 7000);pwm_duty(PWMA_CH2P_P62, 0);pwm_duty(PWMA_CH3P_P64, 7000);pwm_duty(PWMA_CH4P_P66, 0);&#125;&#x2F;******************** 车子停止*******************&#x2F;void go_stop(void)&#123;pwm_duty(PWMA_CH1P_P60, 0);pwm_duty(PWMA_CH2P_P62, 0);pwm_duty(PWMA_CH3P_P64, 0);pwm_duty(PWMA_CH4P_P66, 0);&#125;&#x2F;****************************************** OUT1 P27* OUT2 P26* OUT3 P25* OUT4 P24* 如图循迹小车2所示，从左到右分别为OUT1、2、3、4* 检测到黑线或悬空LED灭，输出高电平* 对单片机的IO口状态进行判断，这样可以得到是哪个* 传感器检测到了黑线，然后再* 做出相对应的行为。*****************************************&#x2F;void scan(void)&#123;if ((P26 &#x3D;&#x3D; 0)&amp;&amp;(P25 &#x3D;&#x3D; 0)&amp;&amp;(P27 &#x3D;&#x3D; 0)&amp;&amp;(P24 &#x3D;&#x3D; 0))&#123;&#x2F;&#x2F; 未检测到黑线go_straight();&#125;else if ((P26 &#x3D;&#x3D; 1)&amp;&amp;(P25 &#x3D;&#x3D; 1)&amp;&amp;(P27 &#x3D;&#x3D; 1)&amp;&amp;(P24 &#x3D;&#x3D; 1))&#123;&#x2F;&#x2F; 悬空go_stop();&#125;else if ((P26 &#x3D;&#x3D; 0)&amp;&amp;(P25 &#x3D;&#x3D; 1)&amp;&amp;(P27 &#x3D;&#x3D; 0)&amp;&amp;(P24 &#x3D;&#x3D; 0))&#123;&#x2F;&#x2F; 车头右偏（相对于前进方向） 向左转（幅度较小）turn_left(7000);&#125;else if ((P26 &#x3D;&#x3D; 1)&amp;&amp;(P25 &#x3D;&#x3D; 0)&amp;&amp;(P27 &#x3D;&#x3D; 0)&amp;&amp;(P24 &#x3D;&#x3D; 0))&#123;&#x2F;&#x2F; 车头左偏 向右转（幅度较小）turn_right(7000);&#125;else if ((P26 &#x3D;&#x3D; 0)&amp;&amp;(P25 &#x3D;&#x3D; 0)&amp;&amp;(P27 &#x3D;&#x3D; 1)&amp;&amp;(P24 &#x3D;&#x3D; 0))&#123;&#x2F;&#x2F; 向左转（幅度较大）turn_left(9500);&#125;else if ((P26 &#x3D;&#x3D; 0)&amp;&amp;(P25 &#x3D;&#x3D; 0)&amp;&amp;(P27 &#x3D;&#x3D; 0)&amp;&amp;(P24 &#x3D;&#x3D; 1))&#123;&#x2F;&#x2F; 向右转（幅度较大）turn_right(9500);&#125;&#125;void main()&#123;DisableGlobalIRQ();&#x2F;&#x2F;关闭总中断board_init();&#x2F;&#x2F;初始化寄存器All_Init();&#x2F;&#x2F;总中断最后开启EnableGlobalIRQ();&#x2F;&#x2F;开启总中断    while(1)&#123;scan();  &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>程序代码大概就像上面那样。其中有些参数，比如说小车行进速度和转弯占空比该给多少。这个东西需要自己测试然后填写，也就是自己慢慢调参测试，找到合适的数值就行。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 单片机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4路红外循迹模块使用教程</title>
      <link href="posts/2.html"/>
      <url>posts/2.html</url>
      
        <content type="html"><![CDATA[<h1 id="4路红外循迹模块使用教程"><a href="#4路红外循迹模块使用教程" class="headerlink" title="4路红外循迹模块使用教程"></a>4路红外循迹模块使用教程</h1><a id="more"></a><h2 id="模块详细信息："><a href="#模块详细信息：" class="headerlink" title="模块详细信息："></a>模块详细信息：</h2><p>工作电压：DC 3.3V~5V</p><p>工作电流：尽量选择1A以上电源供电</p><p>工作温度：-10℃~+50℃</p><p>安装孔径：M3螺丝</p><p>检测距离：1mm~30cm可调，距离越近2性能越稳定，白色反射距离远</p><p>尺寸大小：中控板42mmX38mmX12mm（长X宽X高）；小板向前25mmX12mmX12mm（长X宽X高）</p><p>输出接口：6线制接口（1、2、3、4为4路信号输出端，VCC接正电源，GND接GND）</p><p>输出信号：TTL电平，可直接连接单片机IO口，感应到传感器反射回来的红外光时，红指示灯亮，输出低电平；没有红外光时，指示灯不亮，输出高电平。</p><h2 id="模块接线"><a href="#模块接线" class="headerlink" title="模块接线"></a>模块接线</h2><p><img src="/posts/2/1.png" alt="4路红外循迹模块"></p><p>图中主控板左侧分别为，VCC、GND、OUT1、OUT2、OUT3、OUT4，其中OUT1、2、3、4分别连接单片机的IO口，用于检测输出电平。主控板右侧分别与四个小板连接，用于采集信息。</p><h2 id="模块使用"><a href="#模块使用" class="headerlink" title="模块使用"></a>模块使用</h2><p>模块正确连接并通电后，小板传感器开始工作。模块感应到传感器反射回来的红外光时，红指示灯亮，输出低电平；没有红外光时，指示灯不亮，输出高电平。以检测黑线为例，如果传感器检测到黑线，红外光无法反射回来则模块红指示灯熄灭，同时小板对应的OUT口输出高电平；如果未检测到黑线，红外光可以反射回来则模块红指示灯亮，小板对应的OUT口输出低电平。其中需要注意一种特殊情况，也就是说当测量物体超出测量范围时，此时红指示灯熄灭，OUT口输出高电平。</p><p>每个小板前面对应的电位器可以用来调整使用精度。</p><h2 id="相关代码"><a href="#相关代码" class="headerlink" title="相关代码"></a>相关代码</h2><p>代码使用的单片机型号：STC16F40K128</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">#include &quot;headfile.h&quot;&#x2F;** OUT1 P27* OUT2 P26* OUT3 P25* OUT4 P24* 检测到黑线或悬空LED灭，输出高电平*&#x2F; int main() &#123;DisableGlobalIRQ();&#x2F;&#x2F;关闭总中断board_init();&#x2F;&#x2F;初始化寄存器&#x2F;&#x2F;总中断最后开启EnableGlobalIRQ();&#x2F;&#x2F;开启总中断    while(1)&#123;        &#x2F;&#x2F; 判断IO口电平状态if (P27 &#x3D;&#x3D; 1)        &#123;printf(&quot;OUT1 NO\r\n&quot;);&#125;if (P26 &#x3D;&#x3D; 1)&#123;printf(&quot;OUT2 NO\r\n&quot;);&#125;if (P25 &#x3D;&#x3D; 1)&#123;printf(&quot;OUT3 NO\r\n&quot;);&#125;if (P24 &#x3D;&#x3D; 1)&#123;printf(&quot;OUT4 NO\r\n&quot;);&#125;if ((P27 &#x3D;&#x3D; 0)&amp;&amp;(P26 &#x3D;&#x3D; 0)&amp;&amp;(P25 &#x3D;&#x3D; 0)&amp;&amp;(P24 &#x3D;&#x3D; 0))&#123;printf(&quot;OFF\r\n&quot;);&#125;  &#125; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码是比较简单的，就是判断单片机IO口状态。如果IO口是高电平则表示传感器检测到黑线，否则没有检测到黑线。理解了这个原理就可以在其他型号的单片机上使用这个模块。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 硬件使用 </tag>
            
            <tag> 单片机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown学习笔记</title>
      <link href="posts/1.html"/>
      <url>posts/1.html</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown学习笔记"><a href="#Markdown学习笔记" class="headerlink" title="Markdown学习笔记"></a>Markdown学习笔记</h1><a id="more"></a><h2 id="1、代码块"><a href="#1、代码块" class="headerlink" title="1、代码块"></a>1、代码块</h2><p><strong>代码块语法：</strong></p><p>“ ``` ” 三个这个号（ESC下面的）加上想写的语言，就可以生成代码块区域</p><h2 id="2、标题"><a href="#2、标题" class="headerlink" title="2、标题"></a>2、标题</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><h2 id="3、字体"><a href="#3、字体" class="headerlink" title="3、字体"></a>3、字体</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 加粗**闪电丶教主**# 代码高亮显示&#x3D;&#x3D;闪电丶教主&#x3D;&#x3D;# 删除线~~闪电丶教主~~# 斜体*闪电丶教主*<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="加粗"><a href="#加粗" class="headerlink" title="加粗"></a>加粗</h3><p><strong>闪电丶教主</strong></p><h3 id="代码高亮显示"><a href="#代码高亮显示" class="headerlink" title="代码高亮显示"></a>代码高亮显示</h3><p><code>闪电丶教主</code></p><h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><p><del>闪电丶教主</del></p><h3 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h3><p><em>闪电丶教主</em></p><h2 id="4、引用"><a href="#4、引用" class="headerlink" title="4、引用"></a>4、引用</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 引用语法&gt;作者：闪电丶教主&gt;&gt;作者：闪电丶教主&gt;&gt;&gt;作者：闪电丶教主<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>作者：闪电丶教主</p><blockquote><p>作者：闪电丶教主</p></blockquote><blockquote><blockquote><p>作者：闪电丶教主</p></blockquote></blockquote></blockquote><h2 id="5、分割线"><a href="#5、分割线" class="headerlink" title="5、分割线"></a>5、分割线</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 分割线 比较常用---# 分割线2***<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><hr><h2 id="6、图片插入"><a href="#6、图片插入" class="headerlink" title="6、图片插入"></a>6、图片插入</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 在线图片 # 本地图片![图片名称](图片路径)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/posts/1/666.png" alt="测试图片"></p><p><img src="/posts/1/%E5%B0%8F%E6%81%B6%E9%AD%94.png" alt="就这¿"></p><h2 id="7、超链接"><a href="#7、超链接" class="headerlink" title="7、超链接"></a>7、超链接</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 超链接[名称](地址)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="8、列表语法"><a href="#8、列表语法" class="headerlink" title="8、列表语法"></a>8、列表语法</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 无序列表- 目录1- 目录2- 目录3# 1+. +名称1. 闪电丶教主<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>目录1</li><li>目录2</li><li>目录3</li></ul><ol><li>闪电丶教主</li></ol><h2 id="9、表格"><a href="#9、表格" class="headerlink" title="9、表格"></a>9、表格</h2><table><thead><tr><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>6</td><td>6</td><td>6</td></tr><tr><td>6</td><td>6</td><td>6</td></tr><tr><td>6</td><td>6</td><td>6</td></tr></tbody></table><p>ctrl+/ 查看源代码</p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
